{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import EoN\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import datetime\n",
    "import scipy.stats\n",
    "from matplotlib.dates import (YEARLY, DateFormatter,\n",
    "                              rrulewrapper, RRuleLocator, drange)\n",
    "\n",
    "import seaborn as sns\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "inline_rc = dict(mpl.rcParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for update the inforamation of each node\n",
    "def update_time(reulst, G, N, upper_bound = 10000):\n",
    "    for i in range(N):\n",
    "        if G._node[i]['status'] == \"S\":\n",
    "            for j in range(1, len(reulst.node_history(i)[0])):\n",
    "                if reulst.node_history(i)[0][j] <= upper_bound:\n",
    "                    G._node[i].update({reulst.node_history(i)[1][j]: \n",
    "                                       reulst.node_history(i)[0][j]})\n",
    "        elif G._node[i]['status'] == \"E\":\n",
    "            for j in range(1, len(reulst.node_history(i)[0])):\n",
    "                if reulst.node_history(i)[0][j] <= upper_bound:\n",
    "                    G._node[i].update({reulst.node_history(i)[1][j]: \n",
    "                                       reulst.node_history(i)[0][j]})\n",
    "        elif G._node[i]['status'] == \"I\":\n",
    "            for j in range(1, len(reulst.node_history(i)[0])):\n",
    "                if reulst.node_history(i)[0][j] <= upper_bound:\n",
    "                    G._node[i].update({reulst.node_history(i)[1][j]: \n",
    "                                       reulst.node_history(i)[0][j]})\n",
    "                                   \n",
    "def update_status(G, N):\n",
    "    for i in range(N):\n",
    "        if G._node[i]['R']> -0.1:\n",
    "            G._node[i].update({'status': \"R\"})\n",
    "        elif G._node[i]['I']> - 0.1:\n",
    "            G._node[i].update({'status': \"I\"})\n",
    "        elif G._node[i]['E'] > -0.1:\n",
    "             G._node[i].update({'status': \"E\"})\n",
    "            \n",
    "def update_from(result, G, upper_bound = 10000):\n",
    "    for i in range(len(result.transmissions())):\n",
    "        j = result.transmissions()[i][2]\n",
    "        if G._node[j]['From'] == -1:\n",
    "            if result.transmissions()[i][0]<= upper_bound:\n",
    "                G._node[j].update({'From': result.transmissions()[i][1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to analyze the list\n",
    "def mean_var_std(list_aim):\n",
    "    return(round(np.mean(list_aim),3),\n",
    "           round(np.var(list_aim),3),\n",
    "           round(np.std(list_aim),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def N_average_move(input_list, average_num):\n",
    "    cumsum = [0]\n",
    "    if average_num == 3:\n",
    "        moving_aves = [sum(input_list[:2])/2]\n",
    "    if average_num == 5:\n",
    "        moving_aves = [sum(input_list[:3])/3]\n",
    "        moving_aves.append(sum(input_list[:4])/4)\n",
    "    if average_num == 7:\n",
    "        moving_aves = [sum(input_list[:4])/4]\n",
    "        moving_aves.append(sum(input_list[:5])/5)\n",
    "        moving_aves.append(sum(input_list[:6])/6)\n",
    "    if average_num == 9:\n",
    "        moving_aves = [sum(input_list[:5])/5]\n",
    "        moving_aves.append(sum(input_list[:6])/6)\n",
    "        moving_aves.append(sum(input_list[:7])/7)\n",
    "        moving_aves.append(sum(input_list[:8])/8)\n",
    "    for i, x in enumerate(input_list, 1):\n",
    "        cumsum.append(cumsum[i-1] + x)\n",
    "        if i>=average_num:\n",
    "            moving_ave = (cumsum[i] - cumsum[i-average_num])/average_num\n",
    "            #can do stuff with moving_ave here\n",
    "            moving_aves.append(moving_ave)\n",
    "    if average_num == 3:\n",
    "        moving_aves.append(sum(input_list[-2:])/2)\n",
    "    elif average_num == 5:\n",
    "        moving_aves.append(sum(input_list[-4:])/4)\n",
    "        moving_aves.append(sum(input_list[-3:])/3)\n",
    "    elif average_num == 7:\n",
    "        moving_aves.append(sum(input_list[-6:])/6)\n",
    "        moving_aves.append(sum(input_list[-5:])/5)\n",
    "        moving_aves.append(sum(input_list[-4:])/4)\n",
    "    else:\n",
    "        moving_aves.append(sum(input_list[-8:])/8)\n",
    "        moving_aves.append(sum(input_list[-7:])/7)\n",
    "        moving_aves.append(sum(input_list[-6:])/6)\n",
    "        moving_aves.append(sum(input_list[-5:])/5)\n",
    "    \n",
    "    return(moving_aves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the figure\n",
    "def plot_mean_uppper_lower (fgure_for_plot, t_list_stand, S_list_stand, I_list_stand, finished_time, social_apply_time):\n",
    "    \n",
    "    S_mean = [np.mean(S_list_stand[i][1:]) for i in range(len(S_list_stand))]\n",
    "    S_lowerbound  = [np.quantile(S_list_stand[i][1:], 0.05) for i in range(len(S_list_stand))]\n",
    "    S_upperbound  = [np.quantile(S_list_stand[i][1:], 0.95) for i in range(len(S_list_stand))]\n",
    "    I_mean = [np.mean(I_list_stand[i][1:]) for i in range(len(I_list_stand))]\n",
    "\n",
    "    fi_in =t_list_stand.index(min(t_list_stand,key=lambda x:abs(x-finished_time)))\n",
    "\n",
    "    fgure_for_plot.plot(t_list_stand[:fi_in], S_mean[:fi_in], color = 'C0')\n",
    "    fgure_for_plot.fill_between(t_list_stand[:fi_in], S_lowerbound[:fi_in],S_upperbound[:fi_in], facecolor = 'C0', alpha=0.5,label='90% CI Total Confimred Cases' )\n",
    "    fgure_for_plot.scatter(t_list_stand[fi_in],  S_mean[fi_in], color ='C0', label = 'Total Confimred Cases \\n {} nodes (Mean)'.format(round( S_mean[fi_in],2)))\n",
    "    fgure_for_plot.axvline(x= np.median(social_apply_time), color= 'black', linestyle = '--', label = 'Social Distancing Starts at \\n  {} days (Mean)'.format(round(social_apply_time,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_for_country(number_node, num_neigbor, p_newman_po, family_size):\n",
    "    neigbor = num_neigbor\n",
    "    p_newman = p_newman_po\n",
    "    print(\"generating graph G with {} nodes {} neigbor and {} newconnect\".\n",
    "          format(number_node, neigbor, p_newman))\n",
    "\n",
    "    G_tem = nx.generators.random_graphs.watts_strogatz_graph(number_node, num_neigbor, p_newman_po)\n",
    "\n",
    "    for i in range(number_node):\n",
    "        G_tem._node[i].update({'name':i})\n",
    "        G_tem._node[i].update({'status':'S'})\n",
    "        G_tem._node[i].update({'From': -1})\n",
    "        G_tem._node[i].update({'S': 0})\n",
    "        G_tem._node[i].update({'E': -1})\n",
    "        G_tem._node[i].update({'I': -1})\n",
    "        G_tem._node[i].update({'R': -1})\n",
    "    \n",
    "    country_stable_edges_list = []\n",
    "    country_long_dis_edges_list = []\n",
    "    for i in G_tem.edges:\n",
    "        tem_index = int(i[0]/family_size)\n",
    "        if i[1] in  range(tem_index*family_size,tem_index*family_size+family_size):\n",
    "            country_stable_edges_list.append(i)\n",
    "        else:\n",
    "            country_long_dis_edges_list.append(i)\n",
    "\n",
    "    print(\"num orginal edges: \", len(G_tem.edges))\n",
    "    print(\"num baxic edges: \", len(country_stable_edges_list))\n",
    "    print(\"num long dis edges: \", len(country_long_dis_edges_list))\n",
    "    \n",
    "    return(G_tem,country_stable_edges_list, country_long_dis_edges_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation with the end of the second stage\n",
    "def SEIR_only_social_distancing(G,infected_rate, recoverey_rate,try_end_stage_one, \n",
    "                                stage_one_threshold, G_remove_list, max_time_simulation,quaran='F'):\n",
    "    # the whole recored\n",
    "    combine_t = []\n",
    "    combine_I = []\n",
    "    combine_E = []\n",
    "    combine_S = []\n",
    "    combine_R = []\n",
    "    return_statuses = ('S', 'E', 'I', 'R')\n",
    "    \n",
    "    infrate = 0.019\n",
    "    # build the subgraph\n",
    "    H_pre = nx.DiGraph()\n",
    "    H_pre.add_node('S')\n",
    "    H_pre.add_edge('E', 'I', rate = infected_rate,)\n",
    "    H_pre.add_edge('I', 'R', rate = recoverey_rate)\n",
    "    J_pre = nx.DiGraph()\n",
    "    J_pre.add_edge(('I', 'S'), ('I', 'E'), rate = infrate)\n",
    "    J_pre.add_edge(('E', 'S'), ('E', 'E'), rate = infrate)\n",
    "    \n",
    "    \n",
    "    # set inital input variables\n",
    "    start_time = 0\n",
    "    end_time = try_end_stage_one \n",
    "    inital_status = nx.get_node_attributes(G, 'status')\n",
    "    num_node = len(G._node)\n",
    "    \n",
    "    final_cuc = 0\n",
    "    while final_cuc < round(stage_one_threshold*num_node)+1:\n",
    "        # run simulation in each time_interval before the first time threshold coming\n",
    "        sim_data =  EoN.Gillespie_simple_contagion(G, H_pre, J_pre, inital_status, return_statuses,\n",
    "                                                   tmin = start_time, tmax =end_time,return_full_data=True)\n",
    "        tem_cuc_list = [sim_data.I().tolist()[k]+sim_data.R().tolist()[k] for k in range(len(sim_data.I()))]\n",
    "        final_cuc = tem_cuc_list[-1]\n",
    "    \n",
    "    close_stage_1_time_index = tem_cuc_list.index(round(stage_one_threshold*num_node))+1\n",
    "    close_stage_1_time = sim_data.t()[close_stage_1_time_index-1]\n",
    "    \n",
    "    ## updata information of each node\n",
    "    update_time(sim_data, G, num_node, close_stage_1_time)\n",
    "    update_status(G, num_node)\n",
    "    update_from(sim_data ,G,close_stage_1_time)\n",
    "    \n",
    "    ## record the simiulation result\n",
    "    combine_t = combine_t + sim_data.t().tolist()[:close_stage_1_time_index]\n",
    "    combine_E = combine_E + sim_data.summary()[1]['E'].tolist()[:close_stage_1_time_index]\n",
    "    combine_S = combine_S + sim_data.S().tolist()[:close_stage_1_time_index]\n",
    "    combine_I = combine_I + sim_data.I().tolist()[:close_stage_1_time_index]\n",
    "    combine_R = combine_R + sim_data.R().tolist()[:close_stage_1_time_index]\n",
    "    G_1 =G.copy()\n",
    "    E_case_in_sd = combine_E[-1]\n",
    "\n",
    "\n",
    "    \n",
    "    ## The second stage\n",
    "    # Remove list from remove_list  \n",
    "    for i in G_remove_list:\n",
    "        G.remove_edge(*i)\n",
    "    \n",
    "    inital_status = nx.get_node_attributes(G, 'status')\n",
    "    start_time = close_stage_1_time\n",
    "    \n",
    "    #if quaran == \"T\":\n",
    "     #   J_pre.edges[(('E', 'S'), ('E', 'E'))]['rate']=0.032\n",
    "      #  J_pre.edges[(('I', 'S'), ('I', 'E'))]['rate']=0.032\n",
    "        \n",
    "    # run the simulation of the second stage after removing edges\n",
    "    sim_data =  EoN.Gillespie_simple_contagion(G, H_pre, J_pre, inital_status, return_statuses, tmin = start_time, \n",
    "                                               tmax = max_time_simulation, return_full_data = True)\n",
    "    \n",
    "    # updata information of each node\n",
    "    update_time(sim_data, G, num_node)\n",
    "    update_status(G, num_node)\n",
    "    update_from(sim_data ,G)\n",
    "\n",
    "    # record the simiulation result\n",
    "    combine_t = combine_t + sim_data.t().tolist()\n",
    "    combine_S = combine_S + sim_data.S().tolist()\n",
    "    combine_E = combine_E + sim_data.summary()[1]['E'].tolist()\n",
    "    combine_I = combine_I + sim_data.I().tolist()\n",
    "    combine_R = combine_R + sim_data.R().tolist()\n",
    "    \n",
    "    return(combine_t,combine_S, combine_E, combine_I,combine_R,close_stage_1_time, G_1, G,sim_data, E_case_in_sd )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SEIR_social_case_multi_run(G_tem, country_stable_edges_list, country_long_dis_edges_list, threshold_social_dis,\n",
    "                              policy_power, name, repeat_time, max_time_for_simulation):\n",
    "    # recored each iteration and the time of new infected node\n",
    "    wo_final_cumu = []\n",
    "    wo_infected_peak = [] \n",
    "    wo_infected_peak_time = []\n",
    "    wo_final_stop_time = []\n",
    "    wo_apply_social = []\n",
    "    wo_exposed_number_in_social = []\n",
    "\n",
    "    arr_record_cumu = []\n",
    "    arr_record_I = []\n",
    "    arr_record_R = []\n",
    "\n",
    "    label_plot = ['I', 'R','Cumu']\n",
    "    \n",
    "    ## create a same timeline for all simualtions of benchmark\n",
    "    pre_t_standard = [d for d in np.arange(0, max_time_for_simulation+0.2, 0.2)]\n",
    "    arr_record_cumu = []\n",
    "    arr_record_I = []\n",
    "    arr_record_R = []\n",
    "    \n",
    "    ## recored all run result in each time point\n",
    "    for i in pre_t_standard:\n",
    "        arr_record_cumu.append([i])\n",
    "        arr_record_I.append([i])\n",
    "        arr_record_R.append([i])\n",
    "\n",
    "    # reocrd the time\n",
    "    for i in tqdm(range(repeat_time)):\n",
    "        G = G_tem.copy()\n",
    "        N = len(G_tem.nodes())\n",
    "        initial_infected_list = random.sample(range(N), round(N*rho_set))\n",
    "        for i in initial_infected_list:\n",
    "            G._node[i]['I'] = 0\n",
    "            G._node[i]['status'] = 'I'\n",
    "            G._node[i]['From'] = None\n",
    "\n",
    "        prob_edges_remove =policy_power\n",
    "        # random pick remove list from non-neighbor edages   \n",
    "        G_country_remove_list = random.sample(country_long_dis_edges_list, \n",
    "                                              round(len(country_long_dis_edges_list)*prob_edges_remove))\n",
    "\n",
    "        result = SEIR_only_social_distancing(G,tau ,gamma,30,threshold_social_dis, \n",
    "                                             G_country_remove_list, max_time_for_simulation,quaran=\"T\")\n",
    "\n",
    "        pre_t = result[0]\n",
    "        pre_S = result[1]\n",
    "        pre_I = result[3]\n",
    "        pre_R = result[4]\n",
    "        result_stage = result[5]\n",
    "        \n",
    "        infected_peak = max(pre_I)\n",
    "        infected_peak_time = pre_t[pre_I.index(infected_peak)]\n",
    "        final_stop_time  = pre_t[-1]\n",
    "\n",
    "        # combine all data following the time line\n",
    "        index_time_line = [pre_t.index(min(pre_t, key=lambda x:abs(x-i))) for i in pre_t_standard] \n",
    "        pre_I_standard = [pre_I[i] for i in index_time_line]\n",
    "        pre_R_standard = [pre_R[i] for i in index_time_line]\n",
    "\n",
    "        for i in range(len(pre_t_standard)):\n",
    "            arr_record_cumu[i].append(pre_I_standard[i]+pre_R_standard[i])\n",
    "            arr_record_I[i].append(pre_I_standard[i])\n",
    "            arr_record_R[i].append(pre_R_standard[i])\n",
    "\n",
    "        wo_final_cumu.append(pre_I[-1]+pre_R[-1] ) \n",
    "        wo_infected_peak.append(infected_peak)\n",
    "        wo_infected_peak_time.append(infected_peak_time) \n",
    "        wo_final_stop_time.append(final_stop_time)\n",
    "        wo_apply_social.append(result_stage)\n",
    "        wo_exposed_number_in_social.append(result[9])\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize = (6,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    plot_mean_uppper_lower (ax, pre_t_standard, arr_record_cumu, arr_record_I, \n",
    "                            max_time_for_simulation,np.mean(wo_apply_social))\n",
    "\n",
    "    \n",
    "    ax.set_xlabel(\"Time\",fontsize=14)\n",
    "    ax.set_ylabel(\"Cases\",fontsize=14)\n",
    "    ax.set_title(\"{}: apply {} policy power when {}% cumulative cases\".format(name,round(policy_power*100), round(threshold_social_dis*100,2)))\n",
    "    #plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    plt.legend(loc='lower right',fontsize = 11)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"The cumulative case: \", mean_var_std(wo_final_cumu))\n",
    "    print(\"The infected peak: \", mean_var_std(wo_infected_peak))\n",
    "    print(\"The time of infected peak:\", mean_var_std(wo_infected_peak_time))\n",
    "    print(\"The time of policy application:\", mean_var_std(wo_apply_social))\n",
    "    print(\"The exposed case when policy application:\", mean_var_std(wo_exposed_number_in_social))\n",
    "        \n",
    "    return(pre_t_standard, arr_record_cumu, arr_record_I,wo_apply_social,wo_exposed_number_in_social)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation without the NPI\n",
    "def SEIR_only_social_distancing_ps(G,infected_rate, recoverey_rate, max_time_simulation,quaran='F'):\n",
    "    # the whole recored\n",
    "    combine_t = []\n",
    "    combine_I = []\n",
    "    combine_E = []\n",
    "    combine_S = []\n",
    "    combine_R = []\n",
    "    return_statuses = ('S', 'E', 'I', 'R')\n",
    "    \n",
    "    infrate = 0.019\n",
    "    # build the subgraph\n",
    "    H_pre = nx.DiGraph()\n",
    "    H_pre.add_node('S')\n",
    "    H_pre.add_edge('E', 'I', rate = infected_rate)\n",
    "    H_pre.add_edge('I', 'R', rate = recoverey_rate)\n",
    "    J_pre = nx.DiGraph()\n",
    "    J_pre.add_edge(('I', 'S'), ('I', 'E'), rate = infrate)\n",
    "    J_pre.add_edge(('E', 'S'), ('E', 'E'), rate = infrate)\n",
    "    \n",
    "    \n",
    "    # set inital input variables\n",
    "    num_node = len(G._node)       \n",
    "    inital_status = nx.get_node_attributes(G, 'status')\n",
    "    start_time = 0\n",
    "    \n",
    "        \n",
    "    # run the simulation of the second stage after removing edges\n",
    "    sim_data =  EoN.Gillespie_simple_contagion(G, H_pre, J_pre, inital_status, return_statuses, tmin = start_time, \n",
    "                                               tmax = max_time_simulation, return_full_data = True)\n",
    "    \n",
    "    # updata information of each node\n",
    "    update_time(sim_data, G, num_node)\n",
    "    update_status(G, num_node)\n",
    "    update_from(sim_data ,G)\n",
    "\n",
    "    # record the simiulation result\n",
    "    combine_t = combine_t + sim_data.t().tolist()\n",
    "    combine_S = combine_S + sim_data.S().tolist()\n",
    "    combine_E = combine_E + sim_data.summary()[1]['E'].tolist()\n",
    "    combine_I = combine_I + sim_data.I().tolist()\n",
    "    combine_R = combine_R + sim_data.R().tolist()\n",
    "    \n",
    "    return(combine_t,combine_S, combine_E, combine_I,combine_R, G,sim_data )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SEIR_social_case_multi_run_ps(G_tem, country_stable_edges_list, country_long_dis_edges_list, \n",
    "                                  name, repeat_time, max_time_for_simulation):\n",
    "    # recored each iteration and the time of new infected node\n",
    "    wo_final_cumu = []\n",
    "    wo_infected_peak = [] \n",
    "    wo_infected_peak_time = []\n",
    "    wo_final_stop_time = []\n",
    "\n",
    "    arr_record_cumu = []\n",
    "    arr_record_I = []\n",
    "    arr_record_R = []\n",
    "\n",
    "    label_plot = ['I', 'R','Cumu']\n",
    "    \n",
    "    ## create a same timeline for all simualtions of benchmark\n",
    "    pre_t_standard = [d for d in np.arange(0, max_time_for_simulation+0.2, 0.2)]\n",
    "    arr_record_cumu = []\n",
    "    arr_record_I = []\n",
    "    arr_record_R = []\n",
    "    \n",
    "    ## recored all run result in each time point\n",
    "    for i in pre_t_standard:\n",
    "        arr_record_cumu.append([i])\n",
    "        arr_record_I.append([i])\n",
    "        arr_record_R.append([i])\n",
    "\n",
    "    # reocrd the time\n",
    "    for i in tqdm(range(repeat_time)):\n",
    "        G = G_tem.copy()\n",
    "        N = len(G_tem.nodes())\n",
    "        initial_infected_list = random.sample(range(N), round(N*rho_set))\n",
    "        for i in initial_infected_list:\n",
    "            G._node[i]['I'] = 0\n",
    "            G._node[i]['status'] = 'I'\n",
    "            G._node[i]['From'] = None\n",
    "\n",
    "\n",
    "        result = SEIR_only_social_distancing_ps(G,tau ,gamma,max_time_for_simulation,quaran=\"F\")\n",
    "\n",
    "        pre_t = result[0]\n",
    "        pre_S = result[1]\n",
    "        pre_I = result[3]\n",
    "        pre_R = result[4]\n",
    "        \n",
    "        infected_peak = max(pre_I)\n",
    "        infected_peak_time = pre_t[pre_I.index(infected_peak)]\n",
    "        final_stop_time  = pre_t[-1]\n",
    "\n",
    "        # combine all data following the time line\n",
    "        index_time_line = [pre_t.index(min(pre_t, key=lambda x:abs(x-i))) for i in pre_t_standard] \n",
    "        pre_I_standard = [pre_I[i] for i in index_time_line]\n",
    "        pre_R_standard = [pre_R[i] for i in index_time_line]\n",
    "\n",
    "        for i in range(len(pre_t_standard)):\n",
    "            arr_record_cumu[i].append(pre_I_standard[i]+pre_R_standard[i])\n",
    "            arr_record_I[i].append(pre_I_standard[i])\n",
    "            arr_record_R[i].append(pre_R_standard[i])\n",
    "\n",
    "        wo_final_cumu.append(pre_I[-1]+pre_R[-1] ) \n",
    "        wo_infected_peak.append(infected_peak)\n",
    "        wo_infected_peak_time.append(infected_peak_time) \n",
    "        wo_final_stop_time.append(final_stop_time)\n",
    "\n",
    "    \n",
    "    print(\"The cumulative case: \", mean_var_std(wo_final_cumu))\n",
    "    print(\"The infected peak: \", mean_var_std(wo_infected_peak))\n",
    "    print(\"The time of infected peak:\", mean_var_std(wo_infected_peak_time))\n",
    "        \n",
    "    return(pre_t_standard, arr_record_cumu, arr_record_I)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation with the end of the second stage\n",
    "def SEIR_only_social_distancing_ps2(G,infected_rate, recoverey_rate,try_end_stage_one, G_remove_list, max_time_simulation,quaran='F'):\n",
    "    # the whole recored\n",
    "    combine_t = []\n",
    "    combine_I = []\n",
    "    combine_E = []\n",
    "    combine_S = []\n",
    "    combine_R = []\n",
    "    return_statuses = ('S', 'E', 'I', 'R')\n",
    "    \n",
    "    infrate = 0.019\n",
    "    # build the subgraph\n",
    "    H_pre = nx.DiGraph()\n",
    "    H_pre.add_node('S')\n",
    "    H_pre.add_edge('E', 'I', rate = infected_rate)\n",
    "    H_pre.add_edge('I', 'R', rate = recoverey_rate)\n",
    "    J_pre = nx.DiGraph()\n",
    "    J_pre.add_edge(('I', 'S'), ('I', 'E'), rate = infrate)\n",
    "    J_pre.add_edge(('E', 'S'), ('E', 'E'), rate = infrate)\n",
    "    \n",
    "    \n",
    "    # set inital input variables\n",
    "    start_time = 0\n",
    "    end_time = try_end_stage_one \n",
    "    inital_status = nx.get_node_attributes(G, 'status')\n",
    "    num_node = len(G._node)\n",
    "    \n",
    "\n",
    "    sim_data =  EoN.Gillespie_simple_contagion(G, H_pre, J_pre, inital_status, return_statuses,\n",
    "                                                   tmin = start_time, tmax =end_time,return_full_data=True)\n",
    "\n",
    "    close_stage_1_time = try_end_stage_one \n",
    "    ## updata information of each node\n",
    "    update_time(sim_data, G, num_node, try_end_stage_one )\n",
    "    update_status(G, num_node)\n",
    "    update_from(sim_data ,G,try_end_stage_one)\n",
    "    \n",
    "    ## record the simiulation result\n",
    "    combine_t = combine_t + sim_data.t().tolist()\n",
    "    combine_E = combine_E + sim_data.summary()[1]['E'].tolist()\n",
    "    combine_S = combine_S + sim_data.S().tolist()\n",
    "    combine_I = combine_I + sim_data.I().tolist()\n",
    "    combine_R = combine_R + sim_data.R().tolist()\n",
    "    G_1 =G.copy()\n",
    "    E_case_in_sd = combine_E[-1]\n",
    "\n",
    "\n",
    "    \n",
    "    ## The second stage\n",
    "    # Remove list from remove_list  \n",
    "    for i in G_remove_list:\n",
    "        G.remove_edge(*i)\n",
    "    \n",
    "    inital_status = nx.get_node_attributes(G, 'status')\n",
    "    start_time = close_stage_1_time\n",
    "    \n",
    "    #if quaran == \"T\":\n",
    "     #   J_pre.edges[(('E', 'S'), ('E', 'E'))]['rate']=0.032\n",
    "      #  J_pre.edges[(('I', 'S'), ('I', 'E'))]['rate']=0.032\n",
    "        \n",
    "    # run the simulation of the second stage after removing edges\n",
    "    sim_data =  EoN.Gillespie_simple_contagion(G, H_pre, J_pre, inital_status, return_statuses, tmin = start_time, \n",
    "                                               tmax = max_time_simulation, return_full_data = True)\n",
    "    \n",
    "    # updata information of each node\n",
    "    update_time(sim_data, G, num_node)\n",
    "    update_status(G, num_node)\n",
    "    update_from(sim_data ,G)\n",
    "\n",
    "    # record the simiulation result\n",
    "    combine_t = combine_t + sim_data.t().tolist()\n",
    "    combine_S = combine_S + sim_data.S().tolist()\n",
    "    combine_E = combine_E + sim_data.summary()[1]['E'].tolist()\n",
    "    combine_I = combine_I + sim_data.I().tolist()\n",
    "    combine_R = combine_R + sim_data.R().tolist()\n",
    "    \n",
    "    return(combine_t,combine_S, combine_E, combine_I,combine_R,close_stage_1_time, G_1, G,sim_data, E_case_in_sd )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SEIR_social_case_multi_run_ps2(G_tem, country_stable_edges_list, country_long_dis_edges_list, threshold_time,\n",
    "                              policy_power, name, repeat_time, max_time_for_simulation):\n",
    "    # recored each iteration and the time of new infected node\n",
    "    wo_final_cumu = []\n",
    "    wo_infected_peak = [] \n",
    "    wo_infected_peak_time = []\n",
    "    wo_final_stop_time = []\n",
    "    wo_apply_social = []\n",
    "    wo_exposed_number_in_social = []\n",
    "\n",
    "    arr_record_cumu = []\n",
    "    arr_record_I = []\n",
    "    arr_record_R = []\n",
    "\n",
    "    label_plot = ['I', 'R','Cumu']\n",
    "    \n",
    "    ## create a same timeline for all simualtions of benchmark\n",
    "    pre_t_standard = [d for d in np.arange(0, max_time_for_simulation+0.2, 0.2)]\n",
    "    arr_record_cumu = []\n",
    "    arr_record_I = []\n",
    "    arr_record_R = []\n",
    "    \n",
    "    ## recored all run result in each time point\n",
    "    for i in pre_t_standard:\n",
    "        arr_record_cumu.append([i])\n",
    "        arr_record_I.append([i])\n",
    "        arr_record_R.append([i])\n",
    "\n",
    "    # reocrd the time\n",
    "    for i in tqdm(range(repeat_time)):\n",
    "        G = G_tem.copy()\n",
    "        N = len(G_tem.nodes())\n",
    "        initial_infected_list = random.sample(range(N), round(N*rho_set))\n",
    "        for i in initial_infected_list:\n",
    "            G._node[i]['I'] = 0\n",
    "            G._node[i]['status'] = 'I'\n",
    "            G._node[i]['From'] = None\n",
    "\n",
    "        prob_edges_remove =policy_power\n",
    "        # random pick remove list from non-neighbor edages   \n",
    "        G_country_remove_list = random.sample(country_long_dis_edges_list, \n",
    "                                              round(len(country_long_dis_edges_list)*prob_edges_remove))\n",
    "\n",
    "        result = SEIR_only_social_distancing_ps2(G,tau ,gamma,threshold_time,\n",
    "                                             G_country_remove_list, max_time_for_simulation,quaran=\"T\")\n",
    "\n",
    "        pre_t = result[0]\n",
    "        pre_S = result[1]\n",
    "        pre_I = result[3]\n",
    "        pre_R = result[4]\n",
    "        result_stage = result[5]\n",
    "        \n",
    "        infected_peak = max(pre_I)\n",
    "        infected_peak_time = pre_t[pre_I.index(infected_peak)]\n",
    "        final_stop_time  = pre_t[-1]\n",
    "\n",
    "        # combine all data following the time line\n",
    "        index_time_line = [pre_t.index(min(pre_t, key=lambda x:abs(x-i))) for i in pre_t_standard] \n",
    "        pre_I_standard = [pre_I[i] for i in index_time_line]\n",
    "        pre_R_standard = [pre_R[i] for i in index_time_line]\n",
    "\n",
    "        for i in range(len(pre_t_standard)):\n",
    "            arr_record_cumu[i].append(pre_I_standard[i]+pre_R_standard[i])\n",
    "            arr_record_I[i].append(pre_I_standard[i])\n",
    "            arr_record_R[i].append(pre_R_standard[i])\n",
    "\n",
    "        wo_final_cumu.append(pre_I[-1]+pre_R[-1] ) \n",
    "        wo_infected_peak.append(infected_peak)\n",
    "        wo_infected_peak_time.append(infected_peak_time) \n",
    "        wo_final_stop_time.append(final_stop_time)\n",
    "        wo_apply_social.append(result_stage)\n",
    "        wo_exposed_number_in_social.append(result[9])\n",
    "\n",
    "    \n",
    "    print(\"The cumulative case: \", mean_var_std(wo_final_cumu))\n",
    "    print(\"The infected peak: \", mean_var_std(wo_infected_peak))\n",
    "    print(\"The time of infected peak:\", mean_var_std(wo_infected_peak_time))\n",
    "    print(\"The time of policy application:\", mean_var_std(wo_apply_social))\n",
    "    print(\"The exposed case when policy application:\", mean_var_std(wo_exposed_number_in_social))\n",
    "        \n",
    "    return(pre_t_standard, arr_record_cumu, arr_record_I,wo_apply_social,wo_exposed_number_in_social)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of nodes\n",
    "N =10000 \n",
    "\n",
    "# initial infected fraction, 1 agent\n",
    "rho_set = 0.0001\n",
    "\n",
    "#transmission rate\n",
    "tau = 1/7\n",
    "# the expected connecting time with neighbor before infected is 1/tau  \n",
    "# the variance is 1/(tau*tau)\n",
    "\n",
    "#recovery rate\n",
    "gamma = 1/14\n",
    "# the expected recovered time after infected is 1/gamma \n",
    "# the varance is 1/(gamam*gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact-based Networking Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each country, the contact-based network is constructed by the number of the mean reported contacts and the average household size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Germany \n",
    "# 7.96 mean reported contacts and 2.05 average household size\n",
    "G_Germany, Germany_stable_edges_list, Germany_long_dis_edges_list = G_for_country(N, 8, 0.41,3)\n",
    "\n",
    "G_play = G_Germany.copy()\n",
    "for i in Germany_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size\")\n",
    "print(mean_var_std(G_subgraph_node_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ialty\n",
    "# 19.77 mean reported contacts and 2.40 average household size\n",
    "G_It, It_stable_edges_list, It_long_dis_edges_list =  G_for_country(N, 20, 0.31,3)\n",
    "\n",
    "G_play = G_It.copy()\n",
    "for i in  It_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size\")\n",
    "print(mean_var_std(G_subgraph_node_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nethelands\n",
    "# 13.85 mean reported contacts and 2.23 average household size\n",
    "G_Neth, Neth_stable_edges_list, Neth_long_dis_edges_list = G_for_country(N, 14, 0.36,3)\n",
    "G_play = G_Neth.copy()\n",
    "for i in  Neth_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size\")\n",
    "print(mean_var_std(G_subgraph_node_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Belgium\n",
    "# 11.84 mean reported contacts and 2.36 average household size\n",
    "G_Be, Be_stable_edges_list, Be_long_dis_edges_list =  G_for_country(N, 12, 0.32,3)\n",
    "G_play = G_Be.copy()\n",
    "for i in  Be_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size\")\n",
    "print(mean_var_std(G_subgraph_node_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UK \n",
    "# 11.74 mean reported contacts and 2.27 average household size\n",
    "G_UK, UK_stable_edges_list, UK_long_dis_edges_list = G_for_country(N, 12,0.35,3)\n",
    "    \n",
    "G_play = G_UK.copy()\n",
    "for i in UK_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size\")\n",
    "print(mean_var_std(G_subgraph_node_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Simulatie the pandenic of countries in the first 60 days. All modified SEIR models using in the following simulaiton are shown in the function section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## number of run\n",
    "tst = 500\n",
    "## time of each run\n",
    "tim  = 60\n",
    "Be_social_dis = SEIR_social_case_multi_run(G_Be, Be_stable_edges_list, Be_long_dis_edges_list, 0.0032,\n",
    "                              0.8148,'Belgium', tst ,tim)\n",
    "\n",
    "It_social_dis = SEIR_social_case_multi_run(G_It, It_stable_edges_list, It_long_dis_edges_list, 0.0016,\n",
    "                              0.9352,'Italy', tst ,tim)\n",
    "\n",
    "UK_social_dis = SEIR_social_case_multi_run(G_UK, UK_stable_edges_list, UK_long_dis_edges_list, 0.0015,\n",
    "                              0.7963,'United Kingdom', tst , tim)\n",
    "\n",
    "neth_social_dis = SEIR_social_case_multi_run(G_Neth, Neth_stable_edges_list, Neth_long_dis_edges_list, 0.0011,\n",
    "                              0.7963,'Netherlands', tst , tim)\n",
    "\n",
    "Germany_social_dis= SEIR_social_case_multi_run(G_Germany, Germany_stable_edges_list, Germany_long_dis_edges_list, 0.0025,\n",
    "                              0.7685,'Germany', tst , tim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation for Scenario-based Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PA FOR the interatcion with the HTTCR and No, Ealy and late NPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of nodes\n",
    "N =10000 \n",
    "# initial infected fraction, 1 agent\n",
    "rho_set = 0.001\n",
    "#transmission rate\n",
    "tau = 1/7\n",
    "#recovery rate\n",
    "gamma = 1/14\n",
    "## number of run\n",
    "tst = 200\n",
    "## time of each run\n",
    "tim  = 100\n",
    "ax_poli = fig.add_subplot(131)\n",
    "HH_ps = [1.9, 2.9, 3.9]\n",
    "p_ps  = [0.65,0.44, 0.18]\n",
    "\n",
    "NPI_str = 0.9\n",
    "saved_G_HH42 = []\n",
    "saved_GHH42_stable_edges_list= []\n",
    "saved_HH42_long_dis_edges_list = []\n",
    "\n",
    "## Early NPIs\n",
    "NPI_time = 10\n",
    "for HH_picked in HH_ps :\n",
    "    tin = HH_ps.index(HH_picked)\n",
    "    G_HH42, HH42_stable_edges_list,HH42_long_dis_edges_list = G_for_country(N,10, p_ps[tin],4)\n",
    "    saved_G_HH42.append(G_HH42)\n",
    "    saved_GHH42_stable_edges_list.append(HH42_stable_edges_list)\n",
    "    saved_HH42_long_dis_edges_list.append(HH42_long_dis_edges_list)\n",
    "    ## Show the netwrok structure\n",
    "    G_play = G_HH42.copy()\n",
    "    G_play_2 = G_HH42.copy()\n",
    "    for i in HH42_long_dis_edges_list:\n",
    "        G_play.remove_edge(*i)\n",
    "    G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "    G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "    print(\"The distribution of household size: \", mean_var_std(G_subgraph_node_list))\n",
    "    ## simulation\n",
    "    test = SEIR_social_case_multi_run_ps2(G_play_2, HH42_stable_edges_list, HH42_long_dis_edges_list, NPI_time,NPI_str,'test', tst ,tim)\n",
    "\n",
    "    np.save(\"ea_npi\"+level_n[tin], test)\n",
    "    \n",
    "## Late NPIs\n",
    "NPI_time = 25    \n",
    "for HH_picked in HH_ps :\n",
    "    tin = HH_ps.index(HH_picked)\n",
    "    ## used the generated network above\n",
    "    G_HH42 = saved_G_HH42[tin]\n",
    "    HH42_stable_edges_list = saved_GHH42_stable_edges_list[tin]\n",
    "    HH42_long_dis_edges_list = saved_HH42_long_dis_edges_list[tin]\n",
    "    G_play_2 = G_HH42.copy()\n",
    "    ## simulation\n",
    "    test = SEIR_social_case_multi_run_ps2(G_play_2, HH42_stable_edges_list, HH42_long_dis_edges_list, NPI_time,NPI_str,'test', tst ,tim)\n",
    "    np.save(\"la_npi\"+level_n[tin], test)\n",
    "\n",
    "    \n",
    "\n",
    "## No NPIs\n",
    "for HH_picked in HH_ps :\n",
    "    tin = HH_ps.index(HH_picked)\n",
    "    ## used the generated network above\n",
    "    G_HH42 = saved_G_HH42[tin]\n",
    "    HH42_stable_edges_list = saved_GHH42_stable_edges_list[tin]\n",
    "    HH42_long_dis_edges_list = saved_HH42_long_dis_edges_list[tin]\n",
    "    G_play_2 = G_HH42.copy()\n",
    "    ## simulation\n",
    "    test = SEIR_social_case_multi_run_ps(G_play_2 , HH42_stable_edges_list, HH42_long_dis_edges_list, 'test', tst ,tim)\n",
    "    np.save(\"ea_no_npi\"+level_n[tin], test)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PA FOR t_NPI on HTTCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## number of nodes\n",
    "N =10000 \n",
    "# initial infected fraction, 1 agent\n",
    "rho_set = 0.001\n",
    "#transmission rate\n",
    "tau = 1/7\n",
    "#recovery rate\n",
    "gamma = 1/14\n",
    "\n",
    "tst = 200\n",
    "## time of each run\n",
    "tim  = 100\n",
    "## the setting of t_NPI\n",
    "M_ps = [4+0.5*i for i in range(55)]\n",
    "NPI_str = 0.9\n",
    "\n",
    "G_HH42, HH42_stable_edges_list,HH42_long_dis_edges_list = G_for_country(N, 10, 0.65,4)\n",
    "## Show the netwrok structure\n",
    "G_play = G_HH42.copy() \n",
    "for i in HH42_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size: \", mean_var_std(G_subgraph_node_list))\n",
    "\n",
    "low_ratio_re_505 = []\n",
    "low_ratio_re_1005 = []\n",
    "## simulation \n",
    "for M_picked in M_ps :\n",
    "    G_play_2 =G_HH42.copy()\n",
    "    test = SEIR_social_case_multi_run_ps2(G_play_2, HH42_stable_edges_list, HH42_long_dis_edges_list, M_picked, NPI_str,'test', tst ,tim)\n",
    "    \n",
    "    low_ratio_re_505.append(test[1][-51*5][1:])\n",
    "    low_ratio_re_1005.append(test[1][-1][1:])\n",
    "\n",
    "np.save(\"low_ratio_npi_505\", low_ratio_re_505)\n",
    "np.save(\"low_ratio_npi_1005\", low_ratio_re_1005)\n",
    "\n",
    "G_HH42, HH42_stable_edges_list,HH42_long_dis_edges_list = G_for_country(N, 10, 0.18,4)\n",
    "\n",
    "## Show the netwrok structure\n",
    "G_play = G_HH42.copy() \n",
    "for i in HH42_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size: \", mean_var_std(G_subgraph_node_list))\n",
    "\n",
    "high_ratio_re_505 = []\n",
    "high_ratio_re_1005 = []\n",
    "\n",
    "## simulation \n",
    "for M_picked in M_ps :\n",
    "    G_play_2 =G_HH42.copy()\n",
    "    test = SEIR_social_case_multi_run_ps2(G_play_2, HH42_stable_edges_list, HH42_long_dis_edges_list, M_picked, NPI_str,'test', tst ,tim)\n",
    "    high_ratio_re_505.append(test[1][-51*5][1:])\n",
    "    high_ratio_re_1005.append(test[1][-1][1:])\n",
    "    \n",
    "np.save(\"high_ratio_npi_505\", high_ratio_re_505)\n",
    "np.save(\"high_ratio_npi_1005\", high_ratio_re_1005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterfactual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of run\n",
    "tst = 200\n",
    "## time of each run\n",
    "tim  = 60\n",
    "Be_conclusion = [[],[],[],[],[],[]]\n",
    "Be_conclusion_name = ['BE','BE_IT', 'BE_UK','BE_NL','BE_DE']\n",
    "Be_conclusion[0] = SEIR_social_case_multi_run(G_Be, Be_stable_edges_list, Be_long_dis_edges_list, 0.0032,\n",
    "                              0.8148,'Belgium', tst ,tim)\n",
    "Be_conclusion[1]  = SEIR_social_case_multi_run(G_Be, Be_stable_edges_list, Be_long_dis_edges_list, 0.0016,\n",
    "                              0.9352,'Beligum_Italy', tst ,tim)\n",
    "Be_conclusion[2] = SEIR_social_case_multi_run(G_Be, Be_stable_edges_list, Be_long_dis_edges_list, 0.0015,\n",
    "                              0.7963,'Beligum_United Kingdom', tst , tim)\n",
    "Be_conclusion[3] = SEIR_social_case_multi_run(G_Be, Be_stable_edges_list, Be_long_dis_edges_list, 0.0011,\n",
    "                              0.7963,'Beligum_Netherlands', tst , tim)\n",
    "Be_conclusion[4]  = SEIR_social_case_multi_run(G_Be, Be_stable_edges_list, Be_long_dis_edges_list, 0.0025,\n",
    "                              0.7685,'Beligum_Germany', tst , tim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of run\n",
    "tst = 200\n",
    "## time of each run\n",
    "tim  = 60\n",
    "DE_conclusion = [[],[],[],[],[],[]]\n",
    "DE_conclusion_name = ['DE_BE','DE_IT', 'DE_UK','DE_NL','DE']\n",
    "DE_conclusion[0] = SEIR_social_case_multi_run(G_Germany, Germany_stable_edges_list, Germany_long_dis_edges_list, 0.0032,\n",
    "                              0.8148,'Germany_Belgium', tst ,tim)\n",
    "DE_conclusion[1] = SEIR_social_case_multi_run(G_Germany, Germany_stable_edges_list, Germany_long_dis_edges_list, 0.0016,\n",
    "                              0.9352,'Germany_Italy', tst ,tim)\n",
    "DE_conclusion[2]= SEIR_social_case_multi_run(G_Germany, Germany_stable_edges_list, Germany_long_dis_edges_list, 0.0015,\n",
    "                              0.7963,'Germany_United Kingdom', tst , tim)\n",
    "DE_conclusion[3] = SEIR_social_case_multi_run(G_Germany, Germany_stable_edges_list, Germany_long_dis_edges_list, 0.0011,\n",
    "                              0.7963,'Germany_Netherlands', tst , tim)\n",
    "DE_conclusion[4] = SEIR_social_case_multi_run(G_Germany, Germany_stable_edges_list, Germany_long_dis_edges_list, 0.0025,\n",
    "                              0.7685,'Germany', tst , tim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of run\n",
    "tst = 200\n",
    "## time of each run\n",
    "tim  = 60\n",
    "IT_conclusion = [[],[],[],[],[],[]]\n",
    "IT_conclusion_name = ['IT_BE','IT', 'IT_UK','IT_NL','IT_DE']\n",
    "IT_conclusion[0] = SEIR_social_case_multi_run(G_It, It_stable_edges_list, It_long_dis_edges_list, 0.0032,\n",
    "                              0.8148,'It_Belgium', tst ,tim)\n",
    "IT_conclusion[1] = SEIR_social_case_multi_run(G_It, It_stable_edges_list, It_long_dis_edges_list, 0.0016,\n",
    "                              0.9352,'Italy', tst ,tim)\n",
    "IT_conclusion[2] = SEIR_social_case_multi_run(G_It, It_stable_edges_list, It_long_dis_edges_list, 0.0015,\n",
    "                              0.7963,'It_United Kingdom', tst , tim)\n",
    "IT_conclusion[3] = SEIR_social_case_multi_run(G_It, It_stable_edges_list, It_long_dis_edges_list, 0.0011,\n",
    "                              0.7963,'It_Netherlands', tst , tim)\n",
    "IT_conclusion[4]= SEIR_social_case_multi_run(G_It, It_stable_edges_list, It_long_dis_edges_list, 0.0025,\n",
    "                              0.7685,'It_Germany', tst , tim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of run\n",
    "tst = 200\n",
    "## time of each run\n",
    "tim  = 60\n",
    "UK_conclusion = [[],[],[],[],[],[]]\n",
    "UK_conclusion_name = ['UK_BE','UK_IT', 'UK','UK_NL','UK_DE']\n",
    "UK_conclusion[0] = SEIR_social_case_multi_run(G_UK, UK_stable_edges_list, UK_long_dis_edges_list, 0.0032,\n",
    "                              0.8148,'UK_Belgium', tst ,tim)\n",
    "UK_conclusion[1] = SEIR_social_case_multi_run(G_UK, UK_stable_edges_list, UK_long_dis_edges_list, 0.0016,\n",
    "                              0.9352,'UK_Italy', tst ,tim)\n",
    "UK_conclusion[2] = SEIR_social_case_multi_run(G_UK, UK_stable_edges_list, UK_long_dis_edges_list, 0.0015,\n",
    "                              0.7963,'United Kingdom', tst , tim)\n",
    "UK_conclusion[3] = SEIR_social_case_multi_run(G_UK, UK_stable_edges_list, UK_long_dis_edges_list, 0.0011,\n",
    "                              0.7963,'UK_Netherlands', tst , tim)\n",
    "UK_conclusion[4] = SEIR_social_case_multi_run(G_UK, UK_stable_edges_list, UK_long_dis_edges_list, 0.0025,\n",
    "                              0.7685,'UK_Germany', tst , tim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of run\n",
    "tst = 200\n",
    "## time of each run\n",
    "tim  = 60\n",
    "NL_conclusion = [[],[],[],[],[],[]]\n",
    "NL_conclusion_name = ['NL_BE','NL_IT', 'NL_UK','NL','NL_DE']\n",
    "NL_conclusion[0] = SEIR_social_case_multi_run(G_Neth, Neth_stable_edges_list, Neth_long_dis_edges_list, 0.0032,\n",
    "                              0.8148,'NL_Belgium', tst ,tim)\n",
    "NL_conclusion[1] = SEIR_social_case_multi_run(G_Neth, Neth_stable_edges_list, Neth_long_dis_edges_list, 0.0016,\n",
    "                              0.9352,'NL_Italy', tst ,tim)\n",
    "NL_conclusion[2] = SEIR_social_case_multi_run(G_Neth, Neth_stable_edges_list, Neth_long_dis_edges_list, 0.0015,\n",
    "                              0.7963,'NL_United Kingdom', tst , tim)\n",
    "NL_conclusion[3] = SEIR_social_case_multi_run(G_Neth, Neth_stable_edges_list, Neth_long_dis_edges_list, 0.0011,\n",
    "                              0.7963,'Netherlands', tst , tim)\n",
    "NL_conclusion[4] = SEIR_social_case_multi_run(G_Neth, Neth_stable_edges_list, Neth_long_dis_edges_list, 0.0025,\n",
    "                              0.7685,'NL_Germany', tst , tim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conclusion = [Be_conclusion,IT_conclusion,UK_conclusion,NL_conclusion,DE_conclusion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
