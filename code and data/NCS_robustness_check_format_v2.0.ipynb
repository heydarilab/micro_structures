{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import EoN\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import datetime\n",
    "import scipy.stats\n",
    "from matplotlib.dates import (YEARLY, DateFormatter,\n",
    "                              rrulewrapper, RRuleLocator, drange)\n",
    "\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data):\n",
    "    if np.max(data) == np.min(data):\n",
    "        return data\n",
    "    else:\n",
    "        return (data - np.min(data)) / (np.max(data) - np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for update the inforamation of each node\n",
    "def update_time(reulst, G, N, upper_bound = 10000):\n",
    "    for i in range(N):\n",
    "        if G._node[i]['status'] == \"S\":\n",
    "            for j in range(1, len(reulst.node_history(i)[0])):\n",
    "                if reulst.node_history(i)[0][j] <= upper_bound:\n",
    "                    G._node[i].update({reulst.node_history(i)[1][j]: \n",
    "                                       reulst.node_history(i)[0][j]})\n",
    "        elif G._node[i]['status'] == \"E\":\n",
    "            for j in range(1, len(reulst.node_history(i)[0])):\n",
    "                if reulst.node_history(i)[0][j] <= upper_bound:\n",
    "                    G._node[i].update({reulst.node_history(i)[1][j]: \n",
    "                                       reulst.node_history(i)[0][j]})\n",
    "        elif G._node[i]['status'] == \"I\":\n",
    "            for j in range(1, len(reulst.node_history(i)[0])):\n",
    "                if reulst.node_history(i)[0][j] <= upper_bound:\n",
    "                    G._node[i].update({reulst.node_history(i)[1][j]: \n",
    "                                       reulst.node_history(i)[0][j]})\n",
    "                                   \n",
    "def update_status(G, N):\n",
    "    for i in range(N):\n",
    "        if G._node[i]['R']> -0.1:\n",
    "            G._node[i].update({'status': \"R\"})\n",
    "        elif G._node[i]['I']> - 0.1:\n",
    "            G._node[i].update({'status': \"I\"})\n",
    "        elif G._node[i]['E'] > -0.1:\n",
    "             G._node[i].update({'status': \"E\"})\n",
    "            \n",
    "def update_from(result, G, upper_bound = 10000):\n",
    "    for i in range(len(result.transmissions())):\n",
    "        j = result.transmissions()[i][2]\n",
    "        if G._node[j]['From'] == -1:\n",
    "            if result.transmissions()[i][0]<= upper_bound:\n",
    "                G._node[j].update({'From': result.transmissions()[i][1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to analyze the list\n",
    "def mean_var_std(list_aim):\n",
    "    return(round(np.mean(list_aim),3),\n",
    "           round(np.var(list_aim),3),\n",
    "           round(np.std(list_aim),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def N_average_move(input_list, average_num):\n",
    "    cumsum = [0]\n",
    "    if average_num == 3:\n",
    "        moving_aves = [sum(input_list[:2])/2]\n",
    "    if average_num == 5:\n",
    "        moving_aves = [sum(input_list[:3])/3]\n",
    "        moving_aves.append(sum(input_list[:4])/4)\n",
    "    if average_num == 7:\n",
    "        moving_aves = [sum(input_list[:4])/4]\n",
    "        moving_aves.append(sum(input_list[:5])/5)\n",
    "        moving_aves.append(sum(input_list[:6])/6)\n",
    "    if average_num == 9:\n",
    "        moving_aves = [sum(input_list[:5])/5]\n",
    "        moving_aves.append(sum(input_list[:6])/6)\n",
    "        moving_aves.append(sum(input_list[:7])/7)\n",
    "        moving_aves.append(sum(input_list[:8])/8)\n",
    "    for i, x in enumerate(input_list, 1):\n",
    "        cumsum.append(cumsum[i-1] + x)\n",
    "        if i>=average_num:\n",
    "            moving_ave = (cumsum[i] - cumsum[i-average_num])/average_num\n",
    "            #can do stuff with moving_ave here\n",
    "            moving_aves.append(moving_ave)\n",
    "    if average_num == 3:\n",
    "        moving_aves.append(sum(input_list[-2:])/2)\n",
    "    elif average_num == 5:\n",
    "        moving_aves.append(sum(input_list[-4:])/4)\n",
    "        moving_aves.append(sum(input_list[-3:])/3)\n",
    "    elif average_num == 7:\n",
    "        moving_aves.append(sum(input_list[-6:])/6)\n",
    "        moving_aves.append(sum(input_list[-5:])/5)\n",
    "        moving_aves.append(sum(input_list[-4:])/4)\n",
    "    else:\n",
    "        moving_aves.append(sum(input_list[-8:])/8)\n",
    "        moving_aves.append(sum(input_list[-7:])/7)\n",
    "        moving_aves.append(sum(input_list[-6:])/6)\n",
    "        moving_aves.append(sum(input_list[-5:])/5)\n",
    "    \n",
    "    return(moving_aves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the figure\n",
    "def plot_mean_uppper_lower (fgure_for_plot, t_list_stand, S_list_stand, I_list_stand, finished_time, social_apply_time):\n",
    "    \n",
    "    S_mean = [np.mean(S_list_stand[i][1:]) for i in range(len(S_list_stand))]\n",
    "    S_lowerbound  = [np.quantile(S_list_stand[i][1:], 0.05) for i in range(len(S_list_stand))]\n",
    "    S_upperbound  = [np.quantile(S_list_stand[i][1:], 0.95) for i in range(len(S_list_stand))]\n",
    "    I_mean = [np.mean(I_list_stand[i][1:]) for i in range(len(I_list_stand))]\n",
    "\n",
    "    fi_in =t_list_stand.index(min(t_list_stand,key=lambda x:abs(x-finished_time)))\n",
    "\n",
    "    fgure_for_plot.plot(t_list_stand[:fi_in], S_mean[:fi_in], color = 'C0')\n",
    "    fgure_for_plot.fill_between(t_list_stand[:fi_in], S_lowerbound[:fi_in],S_upperbound[:fi_in], facecolor = 'C0', alpha=0.5,label='90% CI Total Confimred Cases' )\n",
    "    fgure_for_plot.scatter(t_list_stand[fi_in],  S_mean[fi_in], color ='C0', label = 'Total Confimred Cases \\n {} nodes (Mean)'.format(round( S_mean[fi_in],2)))\n",
    "    fgure_for_plot.axvline(x= np.median(social_apply_time), color= 'black', linestyle = '--', label = 'Social Distancing Starts at \\n  {} days (Mean)'.format(round(social_apply_time,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_for_country(number_node, num_neigbor, p_newman_po, family_size):\n",
    "    neigbor = num_neigbor\n",
    "    p_newman = p_newman_po\n",
    "    print(\"generating graph G with {} nodes {} neigbor and {} newconnect\".\n",
    "          format(number_node, neigbor, p_newman))\n",
    "\n",
    "    G_tem = nx.generators.random_graphs.watts_strogatz_graph(number_node, num_neigbor, p_newman_po)\n",
    "\n",
    "    for i in range(number_node):\n",
    "        G_tem._node[i].update({'name':i})\n",
    "        G_tem._node[i].update({'status':'S'})\n",
    "        G_tem._node[i].update({'From': -1})\n",
    "        G_tem._node[i].update({'S': 0})\n",
    "        G_tem._node[i].update({'E': -1})\n",
    "        G_tem._node[i].update({'I': -1})\n",
    "        G_tem._node[i].update({'R': -1})\n",
    "    \n",
    "    country_stable_edges_list = []\n",
    "    country_long_dis_edges_list = []\n",
    "    for i in G_tem.edges:\n",
    "        tem_index = int(i[0]/family_size)\n",
    "        if i[1] in  range(tem_index*family_size,tem_index*family_size+family_size):\n",
    "            country_stable_edges_list.append(i)\n",
    "        else:\n",
    "            country_long_dis_edges_list.append(i)\n",
    "\n",
    "    print(\"num orginal edges: \", len(G_tem.edges))\n",
    "    print(\"num baxic edges: \", len(country_stable_edges_list))\n",
    "    print(\"num long dis edges: \", len(country_long_dis_edges_list))\n",
    "    \n",
    "    return(G_tem,country_stable_edges_list, country_long_dis_edges_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation with the end of the second stage\n",
    "def SEIR_only_social_distancing(G,infrate, infected_rate, recoverey_rate,try_end_stage_one, \n",
    "                                stage_one_threshold, G_remove_list, max_time_simulation,quaran='F'):\n",
    "    # the whole recored\n",
    "    combine_t = []\n",
    "    combine_I = []\n",
    "    combine_E = []\n",
    "    combine_S = []\n",
    "    combine_R = []\n",
    "    return_statuses = ('S', 'E', 'I', 'R')\n",
    "    \n",
    "    # build the subgraph\n",
    "    H_pre = nx.DiGraph()\n",
    "    H_pre.add_node('S')\n",
    "    H_pre.add_edge('E', 'I', rate = infected_rate,)\n",
    "    H_pre.add_edge('I', 'R', rate = recoverey_rate)\n",
    "    J_pre = nx.DiGraph()\n",
    "    J_pre.add_edge(('I', 'S'), ('I', 'E'), rate = infrate)\n",
    "    J_pre.add_edge(('E', 'S'), ('E', 'E'), rate = infrate)\n",
    "    \n",
    "    \n",
    "    # set inital input variables\n",
    "    start_time = 0\n",
    "    end_time = try_end_stage_one \n",
    "    inital_status = nx.get_node_attributes(G, 'status')\n",
    "    num_node = len(G._node)\n",
    "    \n",
    "    final_cuc = 0\n",
    "    while final_cuc < round(stage_one_threshold*num_node)+1:\n",
    "        # run simulation in each time_interval before the first time threshold coming\n",
    "        sim_data =  EoN.Gillespie_simple_contagion(G, H_pre, J_pre, inital_status, return_statuses,\n",
    "                                                   tmin = start_time, tmax =end_time,return_full_data=True)\n",
    "        tem_cuc_list = [sim_data.I().tolist()[k]+sim_data.R().tolist()[k] for k in range(len(sim_data.I()))]\n",
    "        final_cuc = tem_cuc_list[-1]\n",
    "    \n",
    "    close_stage_1_time_index = tem_cuc_list.index(round(stage_one_threshold*num_node))+1\n",
    "    close_stage_1_time = sim_data.t()[close_stage_1_time_index-1]\n",
    "    \n",
    "    ## updata information of each node\n",
    "    update_time(sim_data, G, num_node, close_stage_1_time)\n",
    "    update_status(G, num_node)\n",
    "    update_from(sim_data ,G,close_stage_1_time)\n",
    "    \n",
    "    ## record the simiulation result\n",
    "    combine_t = combine_t + sim_data.t().tolist()[:close_stage_1_time_index]\n",
    "    combine_E = combine_E + sim_data.summary()[1]['E'].tolist()[:close_stage_1_time_index]\n",
    "    combine_S = combine_S + sim_data.S().tolist()[:close_stage_1_time_index]\n",
    "    combine_I = combine_I + sim_data.I().tolist()[:close_stage_1_time_index]\n",
    "    combine_R = combine_R + sim_data.R().tolist()[:close_stage_1_time_index]\n",
    "    G_1 =G.copy()\n",
    "    E_case_in_sd = combine_E[-1]\n",
    "\n",
    "\n",
    "    \n",
    "    ## The second stage\n",
    "    # Remove list from remove_list  \n",
    "    for i in G_remove_list:\n",
    "        G.remove_edge(*i)\n",
    "    \n",
    "    inital_status = nx.get_node_attributes(G, 'status')\n",
    "    start_time = close_stage_1_time\n",
    "    \n",
    "    #if quaran == \"T\":\n",
    "     #   J_pre.edges[(('E', 'S'), ('E', 'E'))]['rate']=0.032\n",
    "      #  J_pre.edges[(('I', 'S'), ('I', 'E'))]['rate']=0.032\n",
    "        \n",
    "    # run the simulation of the second stage after removing edges\n",
    "    sim_data =  EoN.Gillespie_simple_contagion(G, H_pre, J_pre, inital_status, return_statuses, tmin = start_time, \n",
    "                                               tmax = max_time_simulation, return_full_data = True)\n",
    "    \n",
    "    # updata information of each node\n",
    "    update_time(sim_data, G, num_node)\n",
    "    update_status(G, num_node)\n",
    "    update_from(sim_data ,G)\n",
    "\n",
    "    # record the simiulation result\n",
    "    combine_t = combine_t + sim_data.t().tolist()\n",
    "    combine_S = combine_S + sim_data.S().tolist()\n",
    "    combine_E = combine_E + sim_data.summary()[1]['E'].tolist()\n",
    "    combine_I = combine_I + sim_data.I().tolist()\n",
    "    combine_R = combine_R + sim_data.R().tolist()\n",
    "    \n",
    "    return(combine_t,combine_S, combine_E, combine_I,combine_R,close_stage_1_time, G_1, G,sim_data, E_case_in_sd )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SEIR_social_case_multi_run(G_tem, country_stable_edges_list, country_long_dis_edges_list, threshold_social_dis,\n",
    "                              policy_power, name, repeat_time, max_time_for_simulation):\n",
    "    # recored each iteration and the time of new infected node\n",
    "    wo_final_cumu = []\n",
    "    wo_infected_peak = [] \n",
    "    wo_infected_peak_time = []\n",
    "    wo_final_stop_time = []\n",
    "    wo_apply_social = []\n",
    "    wo_exposed_number_in_social = []\n",
    "\n",
    "    arr_record_cumu = []\n",
    "    arr_record_I = []\n",
    "    arr_record_R = []\n",
    "\n",
    "    label_plot = ['I', 'R','Cumu']\n",
    "    \n",
    "    ## create a same timeline for all simualtions of benchmark\n",
    "    pre_t_standard = [d for d in np.arange(0, max_time_for_simulation+0.2, 0.2)]\n",
    "    arr_record_cumu = []\n",
    "    arr_record_I = []\n",
    "    arr_record_R = []\n",
    "    \n",
    "    ## recored all run result in each time point\n",
    "    for i in pre_t_standard:\n",
    "        arr_record_cumu.append([i])\n",
    "        arr_record_I.append([i])\n",
    "        arr_record_R.append([i])\n",
    "\n",
    "    # reocrd the time\n",
    "    for i in range(repeat_time):\n",
    "        G = G_tem.copy()\n",
    "        N = len(G_tem.nodes())\n",
    "        initial_infected_list = random.sample(range(N), round(N*rho_set))\n",
    "        for i in initial_infected_list:\n",
    "            G._node[i]['I'] = 0\n",
    "            G._node[i]['status'] = 'I'\n",
    "            G._node[i]['From'] = None\n",
    "\n",
    "        prob_edges_remove =policy_power\n",
    "        # random pick remove list from non-neighbor edages   \n",
    "        G_country_remove_list = random.sample(country_long_dis_edges_list, \n",
    "                                              round(len(country_long_dis_edges_list)*prob_edges_remove))\n",
    "\n",
    "        result = SEIR_only_social_distancing(G,beta, tau ,gamma,40,threshold_social_dis, \n",
    "                                             G_country_remove_list, max_time_for_simulation,quaran=\"T\")\n",
    "\n",
    "        pre_t = result[0]\n",
    "        pre_S = result[1]\n",
    "        pre_I = result[3]\n",
    "        pre_R = result[4]\n",
    "        result_stage = result[5]\n",
    "        \n",
    "        infected_peak = max(pre_I)\n",
    "        infected_peak_time = pre_t[pre_I.index(infected_peak)]\n",
    "        final_stop_time  = pre_t[-1]\n",
    "\n",
    "        # combine all data following the time line\n",
    "        index_time_line = [pre_t.index(min(pre_t, key=lambda x:abs(x-i))) for i in pre_t_standard] \n",
    "        pre_I_standard = [pre_I[i] for i in index_time_line]\n",
    "        pre_R_standard = [pre_R[i] for i in index_time_line]\n",
    "\n",
    "        for i in range(len(pre_t_standard)):\n",
    "            arr_record_cumu[i].append(pre_I_standard[i]+pre_R_standard[i])\n",
    "            arr_record_I[i].append(pre_I_standard[i])\n",
    "            arr_record_R[i].append(pre_R_standard[i])\n",
    "\n",
    "        wo_final_cumu.append(pre_I[-1]+pre_R[-1] ) \n",
    "        wo_infected_peak.append(infected_peak)\n",
    "        wo_infected_peak_time.append(infected_peak_time) \n",
    "        wo_final_stop_time.append(final_stop_time)\n",
    "        wo_apply_social.append(result_stage)\n",
    "        wo_exposed_number_in_social.append(result[9])\n",
    "\n",
    "\n",
    "#     fig = plt.figure(figsize = (6,4))\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.spines['right'].set_visible(False)\n",
    "#     ax.spines['top'].set_visible(False)\n",
    "#     plot_mean_uppper_lower (ax, pre_t_standard, arr_record_cumu, arr_record_I, \n",
    "#                             max_time_for_simulation,np.mean(wo_apply_social))\n",
    "\n",
    "  \n",
    "#     ax.set_xlabel(\"Time\",fontsize=14)\n",
    "#     ax.set_ylabel(\"Cases\",fontsize=14)\n",
    "#     ax.set_title(\"{}: apply {} policy power when {}% cumulative cases\".format(name,round(policy_power*100), round(threshold_social_dis*100,2)))\n",
    "#     #plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "#     plt.legend(loc='lower right',fontsize = 11)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "    print(\"The cumulative case: \", mean_var_std(wo_final_cumu))\n",
    "#     print(\"The infected peak: \", mean_var_std(wo_infected_peak))\n",
    "#     print(\"The time of infected peak:\", mean_var_std(wo_infected_peak_time))\n",
    "#     print(\"The time of policy application:\", mean_var_std(wo_apply_social))\n",
    "#     print(\"The exposed case when policy application:\", mean_var_std(wo_exposed_number_in_social))\n",
    "        \n",
    "    return(pre_t_standard, arr_record_cumu, arr_record_I,wo_apply_social,wo_exposed_number_in_social)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global stable parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nodes\n",
    "N =10000 \n",
    "# initial infected fraction, 1 agent\n",
    "rho_set = 0.0001\n",
    "\n",
    "#transmission rate\n",
    "tau = 1/7\n",
    "\n",
    "#recovery rate\n",
    "gamma = 1/14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Parameter analysis, M "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Parameter analysis, M = M(1+20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.019\n",
    "\n",
    "# Germany \n",
    "# 7.96 mean reported contacts and 2.05 average household size. \n",
    "G_Germany, Germany_stable_edges_list, Germany_long_dis_edges_list = G_for_country(N, 10, 0.41,3)\n",
    "\n",
    "# Ialty\n",
    "# 19.77 mean reported contacts and 2.40 average household size\n",
    "G_It, It_stable_edges_list, It_long_dis_edges_list =  G_for_country(N, 24, 0.31,3)\n",
    "\n",
    "\n",
    "# Nethelands\n",
    "# 13.85 mean reported contacts and 2.23 average household size\n",
    "G_Neth, Neth_stable_edges_list, Neth_long_dis_edges_list = G_for_country(N, 16, 0.36,3)\n",
    "\n",
    "# Belgium\n",
    "# 11.84 mean reported contacts and 2.36 average household size\n",
    "G_Be, Be_stable_edges_list, Be_long_dis_edges_list =  G_for_country(N, 14, 0.32,3)\n",
    "\n",
    "## UK \n",
    "# 11.74 mean reported contacts and 2.27 average household size\n",
    "G_UK, UK_stable_edges_list, UK_long_dis_edges_list = G_for_country(N, 14,0.35,3)\n",
    "\n",
    "\n",
    "beta= 0.019\n",
    "## number of run\n",
    "tst = 100\n",
    "## time of each run\n",
    "tim  = 60\n",
    "\n",
    "print('Belgium')\n",
    "Be_social_dis = SEIR_social_case_multi_run(G_Be, Be_stable_edges_list, Be_long_dis_edges_list, 0.0032,\n",
    "                              0.8148,'Belgium', tst ,tim)\n",
    "print('Italy')\n",
    "It_social_dis = SEIR_social_case_multi_run(G_It, It_stable_edges_list, It_long_dis_edges_list, 0.0017,\n",
    "                              0.9352,'Italy', tst ,tim)\n",
    "print('United Kingdom')\n",
    "UK_social_dis = SEIR_social_case_multi_run(G_UK, UK_stable_edges_list, UK_long_dis_edges_list, 0.0015,\n",
    "                              0.7963,'United Kingdom', tst , tim)\n",
    "print('Netherlands')\n",
    "neth_social_dis = SEIR_social_case_multi_run(G_Neth, Neth_stable_edges_list, Neth_long_dis_edges_list, 0.0011,\n",
    "                              0.7963,'Netherlands', tst , tim)\n",
    "print('Germany')\n",
    "Germany_social_dis= SEIR_social_case_multi_run(G_Germany, Germany_stable_edges_list, Germany_long_dis_edges_list, 0.0025,\n",
    "                              0.7685,'Germany', tst , tim)\n",
    "    \n",
    "compare_social_dis = [Be_social_dis, It_social_dis,UK_social_dis,neth_social_dis,Germany_social_dis]\n",
    "country_name_list = ['Belgium','Italy','United Kingdom', 'Netherlands','Germany']\n",
    "for i in range(6):\n",
    "    np.save('100_1.2M{}_final.npy'.format(country_name_list[i]),compare_social_dis[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Parameter analysis, M = M(1-20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.019\n",
    "\n",
    "# Germany \n",
    "# 7.96 mean reported contacts and 2.05 average household size. \n",
    "G_Germany, Germany_stable_edges_list, Germany_long_dis_edges_list = G_for_country(N, 6, 0.41,3)\n",
    "\n",
    "# Ialty\n",
    "# 19.77 mean reported contacts and 2.40 average household size\n",
    "G_It, It_stable_edges_list, It_long_dis_edges_list =  G_for_country(N, 16, 0.31,3)\n",
    "\n",
    "# Nethelands\n",
    "# 13.85 mean reported contacts and 2.23 average household size\n",
    "G_Neth, Neth_stable_edges_list, Neth_long_dis_edges_list = G_for_country(N, 12, 0.36,3)\n",
    "\n",
    "# Belgium\n",
    "# 11.84 mean reported contacts and 2.36 average household size\n",
    "G_Be, Be_stable_edges_list, Be_long_dis_edges_list =  G_for_country(N, 10, 0.32,3)\n",
    "\n",
    "## UK \n",
    "# 11.74 mean reported contacts and 2.27 average household size\n",
    "G_UK, UK_stable_edges_list, UK_long_dis_edges_list = G_for_country(N, 10,0.35,3)\n",
    "\n",
    "\n",
    "beta= 0.019\n",
    "## number of run\n",
    "tst = 100\n",
    "## time of each run\n",
    "tim  = 60\n",
    "\n",
    "print('Belgium')\n",
    "Be_social_dis = SEIR_social_case_multi_run(G_Be, Be_stable_edges_list, Be_long_dis_edges_list, 0.0032,\n",
    "                              0.8148,'Belgium', tst ,tim)\n",
    "print('Italy')\n",
    "It_social_dis = SEIR_social_case_multi_run(G_It, It_stable_edges_list, It_long_dis_edges_list, 0.0017,\n",
    "                              0.9352,'Italy', tst ,tim)\n",
    "print('United Kingdom')\n",
    "UK_social_dis = SEIR_social_case_multi_run(G_UK, UK_stable_edges_list, UK_long_dis_edges_list, 0.0015,\n",
    "                              0.7963,'United Kingdom', tst , tim)\n",
    "print('Netherlands')\n",
    "neth_social_dis = SEIR_social_case_multi_run(G_Neth, Neth_stable_edges_list, Neth_long_dis_edges_list, 0.0011,\n",
    "                              0.7963,'Netherlands', tst , tim)\n",
    "print('Germany')\n",
    "Germany_social_dis= SEIR_social_case_multi_run(G_Germany, Germany_stable_edges_list, Germany_long_dis_edges_list, 0.0025,\n",
    "                              0.7685,'Germany', tst , tim)\n",
    "    \n",
    "compare_social_dis = [Be_social_dis, It_social_dis,UK_social_dis,neth_social_dis,Germany_social_dis, poland_social_dis]\n",
    "country_name_list = ['Belgium','Italy','United Kingdom', 'Netherlands','Germany']\n",
    "for i in range(6):\n",
    "    np.save('100_0.8M{}_final.npy'.format(country_name_list[i]),compare_social_dis[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Parameter analysis, M = M(1+50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.019\n",
    "\n",
    "# Germany \n",
    "# 7.96 mean reported contacts and 2.05 average household size. \n",
    "G_Germany, Germany_stable_edges_list, Germany_long_dis_edges_list = G_for_country(N, 12, 0.41,3)\n",
    "\n",
    "# Ialty\n",
    "# 19.77 mean reported contacts and 2.40 average household size\n",
    "G_It, It_stable_edges_list, It_long_dis_edges_list =  G_for_country(N, 30, 0.31,3)\n",
    "\n",
    "# Nethelands\n",
    "# 13.85 mean reported contacts and 2.23 average household size\n",
    "G_Neth, Neth_stable_edges_list, Neth_long_dis_edges_list = G_for_country(N, 20, 0.36,3)\n",
    "\n",
    "# Belgium\n",
    "# 11.84 mean reported contacts and 2.36 average household size\n",
    "G_Be, Be_stable_edges_list, Be_long_dis_edges_list =  G_for_country(N, 18, 0.32,3)\n",
    "\n",
    "## UK \n",
    "# 11.74 mean reported contacts and 2.27 average household size\n",
    "G_UK, UK_stable_edges_list, UK_long_dis_edges_list = G_for_country(N, 18,0.35,3)\n",
    "\n",
    "\n",
    "beta= 0.019\n",
    "## number of run\n",
    "tst = 100\n",
    "## time of each run\n",
    "tim  = 60\n",
    "\n",
    "print('Belgium')\n",
    "Be_social_dis = SEIR_social_case_multi_run(G_Be, Be_stable_edges_list, Be_long_dis_edges_list, 0.0032,\n",
    "                              0.8148,'Belgium', tst ,tim)\n",
    "print('Italy')\n",
    "It_social_dis = SEIR_social_case_multi_run(G_It, It_stable_edges_list, It_long_dis_edges_list, 0.0017,\n",
    "                              0.9352,'Italy', tst ,tim)\n",
    "print('United Kingdom')\n",
    "UK_social_dis = SEIR_social_case_multi_run(G_UK, UK_stable_edges_list, UK_long_dis_edges_list, 0.0015,\n",
    "                              0.7963,'United Kingdom', tst , tim)\n",
    "print('Netherlands')\n",
    "neth_social_dis = SEIR_social_case_multi_run(G_Neth, Neth_stable_edges_list, Neth_long_dis_edges_list, 0.0011,\n",
    "                              0.7963,'Netherlands', tst , tim)\n",
    "print('Germany')\n",
    "Germany_social_dis= SEIR_social_case_multi_run(G_Germany, Germany_stable_edges_list, Germany_long_dis_edges_list, 0.0025,\n",
    "                              0.7685,'Germany', tst , tim)\n",
    "    \n",
    "compare_social_dis = [Be_social_dis, It_social_dis,UK_social_dis,neth_social_dis,Germany_social_dis]\n",
    "country_name_list = ['Belgium','Italy','United Kingdom', 'Netherlands','Germany']\n",
    "for i in range(6):\n",
    "    np.save('100_1.5M{}_final.npy'.format(country_name_list[i]),compare_social_dis[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parameter analysis for Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of nodes\n",
    "N =10000 \n",
    "\n",
    "# initial infected fraction, 1 agent\n",
    "rho_set = 0.0001\n",
    "\n",
    "#transmission rate\n",
    "tau = 1/7\n",
    "# the expected connecting time with neighbor before infected is 1/tau  \n",
    "# the variance is 1/(tau*tau)\n",
    "\n",
    "#recovery rate\n",
    "gamma = 1/14\n",
    "# the expected recovered time after infected is 1/gamma \n",
    "# the varance is 1/(gamam*gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Germany \n",
    "# 7.96 mean reported contacts and 2.05 average household size. \n",
    "G_Germany, Germany_stable_edges_list, Germany_long_dis_edges_list = G_for_country(N, 8, 0.41,3)\n",
    "\n",
    "G_play = G_Germany.copy()\n",
    "for i in Germany_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size\")\n",
    "print(mean_var_std(G_subgraph_node_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ialty\n",
    "# 19.77 mean reported contacts and 2.40 average household size\n",
    "G_It, It_stable_edges_list, It_long_dis_edges_list =  G_for_country(N, 20, 0.31,3)\n",
    "\n",
    "G_play = G_It.copy()\n",
    "for i in  It_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size\")\n",
    "print(mean_var_std(G_subgraph_node_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nethelands\n",
    "# 13.85 mean reported contacts and 2.23 average household size\n",
    "G_Neth, Neth_stable_edges_list, Neth_long_dis_edges_list = G_for_country(N, 14, 0.36,3)\n",
    "G_play = G_Neth.copy()\n",
    "for i in  Neth_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size\")\n",
    "print(mean_var_std(G_subgraph_node_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Belgium\n",
    "# 11.84 mean reported contacts and 2.36 average household size\n",
    "G_Be, Be_stable_edges_list, Be_long_dis_edges_list =  G_for_country(N, 12, 0.32,3)\n",
    "G_play = G_Be.copy()\n",
    "for i in  Be_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size\")\n",
    "print(mean_var_std(G_subgraph_node_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UK \n",
    "# 11.74 mean reported contacts and 2.27 average household size\n",
    "G_UK, UK_stable_edges_list, UK_long_dis_edges_list = G_for_country(N, 12,0.35,3)\n",
    "    \n",
    "G_play = G_UK.copy()\n",
    "for i in UK_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size\")\n",
    "print(mean_var_std(G_subgraph_node_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Data generation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_list =[round(i*0.001+0.010,3) for i in range(21)]\n",
    "## number of run\n",
    "tst = 100\n",
    "## time of each run\n",
    "tim  = 60\n",
    "ro = []\n",
    "for beta_in in beta_list:\n",
    "    print(beta_in)\n",
    "    beta = beta_in\n",
    "    print('Belgium')\n",
    "    Be_social_dis = SEIR_social_case_multi_run(G_Be, Be_stable_edges_list, Be_long_dis_edges_list, 0.0032,\n",
    "                                  0.8148,'Belgium', tst ,tim)\n",
    "    print('Italy')\n",
    "    It_social_dis = SEIR_social_case_multi_run(G_It, It_stable_edges_list, It_long_dis_edges_list, 0.0017,\n",
    "                                  0.9352,'Italy', tst ,tim)\n",
    "    print('United Kingdom')\n",
    "    UK_social_dis = SEIR_social_case_multi_run(G_UK, UK_stable_edges_list, UK_long_dis_edges_list, 0.0015,\n",
    "                                  0.7963,'United Kingdom', tst , tim)\n",
    "    print('Netherlands')\n",
    "    neth_social_dis = SEIR_social_case_multi_run(G_Neth, Neth_stable_edges_list, Neth_long_dis_edges_list, 0.0011,\n",
    "                                  0.7963,'Netherlands', tst , tim)\n",
    "    print('Germany')\n",
    "    Germany_social_dis= SEIR_social_case_multi_run(G_Germany, Germany_stable_edges_list, Germany_long_dis_edges_list, 0.0025,\n",
    "                                  0.7685,'Germany', tst , tim)\n",
    "\n",
    "    tem_compare = [np.mean(Be_social_dis[1][300][1:]), np.mean(It_social_dis[1][300][1:]),\n",
    "                   np.mean(UK_social_dis[1][300][1:]),np.mean(neth_social_dis[1][300][1:]),\n",
    "                   np.mean(Germany_social_dis[1][300][1:])]\n",
    "    ro.append(tem_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Comparing GSI and Social Interaction Trends, using Google Mobility Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = datetime.datetime(2020, 3,1)\n",
    "t_end = datetime.datetime(2020, 5, 1)\n",
    "t_step = datetime.timedelta(1)\n",
    "t_dates = mpl.dates.drange(t_start, t_end, t_step)\n",
    "print(len(t_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name_list = ['Belgium','Italy', 'Germany','Netherlands','United Kingdom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gooogle mobilty data from 02/15/2020 to 2022\n",
    "df = pd.read_csv('Google_Global_Mobility_Report.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GSI track\n",
    "Policy_Power_check = pd.read_csv(\"GSI_and_cases_track_primary_set.csv\",header=0 )\n",
    "df2 = Policy_Power_check[Policy_Power_check['location'].isin(country_name_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name_list = ['Belgium','Italy', 'Germany','Netherlands','United Kingdom']\n",
    "print_country_name_list = ['Belgium','Italy', 'Germany','The Netherlands','The United Kingdom']\n",
    "pea_record = pd.DataFrame(columns = ['country','coefficient', 'p-value'])\n",
    "fig=plt.figure(figsize=(14,8))\n",
    "plot_list = [321+i for i in range(5)]\n",
    "for coun_name in country_name_list:\n",
    "    ax_t = fig.add_subplot(plot_list[country_name_list.index(coun_name)])\n",
    "    df_tag = df[df['country_region'] == coun_name]\n",
    "    df_tag_1= df_tag[[df.columns.tolist()[i] for i in [1,2,4,8,9,10,11,12,13,14]]]\n",
    "    df_tag_f =df_tag_1[df_tag_1['sub_region_1'].isnull()&df_tag_1['metro_area'].isnull()].reset_index(drop=True)\n",
    "    \n",
    "    cam1 = df_tag_f[df.columns.tolist()[14]].rolling(window=7).mean()[15:76].tolist()\n",
    "    cam2 = df2[df2['location'] ==coun_name][61:122]['stringency_index'].tolist()\n",
    "    ax_t.plot_date(t_dates,cam2, label ='GSI',  fmt=\",-\",  c = cmap(1),lw =3, alpha = 0.8)\n",
    "    ax = ax_t.twinx()\n",
    "    ax.plot_date(t_dates,cam1, label = 'Google residential data',  fmt=\",-\",c = cmap(0),lw =3, alpha = 0.8)\n",
    "   \n",
    "\n",
    "    #fig.subplots_adjust(top=0.8)\n",
    "    #ax.legend(loc=\"lower center\",ncol=5,bbox_to_anchor=(0.5, 0.8),bbox_transform=fig.transFigure )\n",
    "    ax.legend(loc=\"lower right\", prop={'size': 14} )\n",
    "    ax_t.legend(loc=\"upper left\", prop={'size': 14} )\n",
    "    ax.set_title(print_country_name_list[country_name_list.index(coun_name)], size =17)\n",
    "    ax.set_ylabel('Residential Percent \\nChange from Baseline',size = 15)\n",
    "    ax_t.set_ylabel('Stringency Index',size = 15)\n",
    "    #formatter = DateFormatter('%m/%d')\n",
    "    #ax.xaxis.set_major_formatter(formatter)\n",
    "    pea_record.loc[len( pea_record)] = [coun_name]+[round(k,5) for k in scipy.stats.pearsonr(cam1, cam2)]\n",
    "    #fig.autofmt_xdate()\n",
    "    #ax.setp(ax.get_xticklabels(), fontsize=11)    \n",
    "    ax.tick_params(axis='y', labelsize= 14)\n",
    "    ax_t.tick_params(axis='y', labelsize= 14)\n",
    "    ax.tick_params(axis='x', labelsize= 15)\n",
    "    \n",
    "fig.autofmt_xdate(rotation=20)\n",
    "fig.tight_layout(rect=[0, 0, 1, 1])\n",
    "#plt.suptitle('Trajectories between the GSI and Google mobility data',size = 18)\n",
    "plt.savefig('googe vs.gsI.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pea_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. average relative error of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 example for primariy Set (5 European Countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### simualted cases\n",
    "country_name_list = ['Belgium','Italy','United Kingdom', 'Netherlands','Germany']\n",
    "simualted_case = []\n",
    "for num in range(5):\n",
    "    a=np.load('500{}_60.npy'.format(country_name_list[num]),allow_pickle=True).tolist()\n",
    "    simualted_case.append(np.mean(a[1][-1][1:]))\n",
    "    print(np.mean(a[1][-1][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## real cases\n",
    "real_track_6 = pd.read_csv(\"GSI_and_cases_track_primary_set\",header=0 )\n",
    "#60 days: 122\n",
    "#90 days: 152\n",
    "#120 days: 182\n",
    "country_name_list = ['Belgium','Italy', 'United Kingdom','Netherlands','Germany']\n",
    "real_case = []\n",
    "for num in range(5):\n",
    "    tem_data = real_track_6[real_track_6['location'] ==country_name_list[num]]\n",
    "    real_case.append(tem_data.iloc[122,5])\n",
    "    print(tem_data.iloc[122,5])\n",
    "print(real_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate the error\n",
    "print(country_name_list)\n",
    "real = real_case\n",
    "print('real case:', real_case)\n",
    "simulated_data =simualted_case\n",
    "print('simluared case:',simualted_case)\n",
    "scaled_coe =29\n",
    "scaled_simulated_data = [i*scaled_coe for i in simulated_data]\n",
    "print('scaled simluared case:',scaled_simulated_data )\n",
    "eror_check = [round(abs(i-j)/i,4) for i,j in zip(real,scaled_simulated_data)]\n",
    "\n",
    "print(eror_check)\n",
    "print(np.mean(eror_check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 example for Validation Set (5 Countries in Eastern/Central Europe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### simualted cases\n",
    "country_name_list = ['Austria', 'Hungary','Slovenia','Slovakia', 'Poland']\n",
    "check  = []\n",
    "for num in range(5):\n",
    "    if num == 4:\n",
    "        a=np.load('500{}_120_new.npy'.format(country_name_list[num]),allow_pickle=True).tolist() \n",
    "    else:\n",
    "        a=np.load('500{}_120.npy'.format(country_name_list[num]),allow_pickle=True).tolist()\n",
    "    check.append(np.mean(a[1][-1][1:]))\n",
    "    print(np.mean(a[1][-1][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## real cases\n",
    "## 60 days    5/7/2020\n",
    "## 90 days    6/7/2020\n",
    "## 120 days    7/7/2020\n",
    "temdf = pd.read_csv('GSI_and_cases_track_validation_set.csv')\n",
    "temdf = temdf[['location','date','total_cases_per_million','stringency_index','population']]\n",
    "tem_re = []\n",
    "for num in range(5):\n",
    "    start = '3/7/2020'\n",
    "    end = '7/7/2020'\n",
    "    tems = temdf[temdf['location'] == country_name_list[num]]['date'].tolist().index(start)\n",
    "    teme = temdf[temdf['location'] == country_name_list[num]]['date'].tolist().index(end)\n",
    "    tem_re.append(temdf[temdf['location'] == country_name_list[num]]['total_cases_per_million'].tolist()[tems:teme][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate the error\n",
    "print(country_name_list)\n",
    "real = tem_re\n",
    "print('real case:', real)\n",
    "simulated_data =check \n",
    "print('simluared case:',simulated_data)\n",
    "scaled_coe =15\n",
    "scaled_simulated_data = [i*scaled_coe for i in simulated_data]\n",
    "print('scaled simluared case:',scaled_simulated_data )\n",
    "eror_check = [round(abs(i-j)/i,4) for i,j in zip(real,scaled_simulated_data)]\n",
    "\n",
    "print(eror_check)\n",
    "print(np.mean(eror_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
