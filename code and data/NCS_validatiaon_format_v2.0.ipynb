{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import EoN\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import datetime\n",
    "import scipy.stats\n",
    "from matplotlib.dates import (YEARLY, DateFormatter,\n",
    "                              rrulewrapper, RRuleLocator, drange)\n",
    "\n",
    "import seaborn as sns\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "inline_rc = dict(mpl.rcParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for update the inforamation of each node\n",
    "def update_time(reulst, G, N, upper_bound = 10000):\n",
    "    for i in range(N):\n",
    "        if G._node[i]['status'] == \"S\":\n",
    "            for j in range(1, len(reulst.node_history(i)[0])):\n",
    "                if reulst.node_history(i)[0][j] <= upper_bound:\n",
    "                    G._node[i].update({reulst.node_history(i)[1][j]: \n",
    "                                       reulst.node_history(i)[0][j]})\n",
    "        elif G._node[i]['status'] == \"E\":\n",
    "            for j in range(1, len(reulst.node_history(i)[0])):\n",
    "                if reulst.node_history(i)[0][j] <= upper_bound:\n",
    "                    G._node[i].update({reulst.node_history(i)[1][j]: \n",
    "                                       reulst.node_history(i)[0][j]})\n",
    "        elif G._node[i]['status'] == \"I\":\n",
    "            for j in range(1, len(reulst.node_history(i)[0])):\n",
    "                if reulst.node_history(i)[0][j] <= upper_bound:\n",
    "                    G._node[i].update({reulst.node_history(i)[1][j]: \n",
    "                                       reulst.node_history(i)[0][j]})\n",
    "                                   \n",
    "def update_status(G, N):\n",
    "    for i in range(N):\n",
    "        if G._node[i]['R']> -0.1:\n",
    "            G._node[i].update({'status': \"R\"})\n",
    "        elif G._node[i]['I']> - 0.1:\n",
    "            G._node[i].update({'status': \"I\"})\n",
    "        elif G._node[i]['E'] > -0.1:\n",
    "             G._node[i].update({'status': \"E\"})\n",
    "            \n",
    "def update_from(result, G, upper_bound = 10000):\n",
    "    for i in range(len(result.transmissions())):\n",
    "        j = result.transmissions()[i][2]\n",
    "        if G._node[j]['From'] == -1:\n",
    "            if result.transmissions()[i][0]<= upper_bound:\n",
    "                G._node[j].update({'From': result.transmissions()[i][1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to analyze the list\n",
    "def mean_var_std(list_aim):\n",
    "    return(round(np.mean(list_aim),3),\n",
    "           round(np.var(list_aim),3),\n",
    "           round(np.std(list_aim),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def N_average_move(input_list, average_num):\n",
    "    cumsum = [0]\n",
    "    if average_num == 3:\n",
    "        moving_aves = [sum(input_list[:2])/2]\n",
    "    if average_num == 5:\n",
    "        moving_aves = [sum(input_list[:3])/3]\n",
    "        moving_aves.append(sum(input_list[:4])/4)\n",
    "    if average_num == 7:\n",
    "        moving_aves = [sum(input_list[:4])/4]\n",
    "        moving_aves.append(sum(input_list[:5])/5)\n",
    "        moving_aves.append(sum(input_list[:6])/6)\n",
    "    if average_num == 9:\n",
    "        moving_aves = [sum(input_list[:5])/5]\n",
    "        moving_aves.append(sum(input_list[:6])/6)\n",
    "        moving_aves.append(sum(input_list[:7])/7)\n",
    "        moving_aves.append(sum(input_list[:8])/8)\n",
    "    for i, x in enumerate(input_list, 1):\n",
    "        cumsum.append(cumsum[i-1] + x)\n",
    "        if i>=average_num:\n",
    "            moving_ave = (cumsum[i] - cumsum[i-average_num])/average_num\n",
    "            #can do stuff with moving_ave here\n",
    "            moving_aves.append(moving_ave)\n",
    "    if average_num == 3:\n",
    "        moving_aves.append(sum(input_list[-2:])/2)\n",
    "    elif average_num == 5:\n",
    "        moving_aves.append(sum(input_list[-4:])/4)\n",
    "        moving_aves.append(sum(input_list[-3:])/3)\n",
    "    elif average_num == 7:\n",
    "        moving_aves.append(sum(input_list[-6:])/6)\n",
    "        moving_aves.append(sum(input_list[-5:])/5)\n",
    "        moving_aves.append(sum(input_list[-4:])/4)\n",
    "    else:\n",
    "        moving_aves.append(sum(input_list[-8:])/8)\n",
    "        moving_aves.append(sum(input_list[-7:])/7)\n",
    "        moving_aves.append(sum(input_list[-6:])/6)\n",
    "        moving_aves.append(sum(input_list[-5:])/5)\n",
    "    \n",
    "    return(moving_aves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the figure\n",
    "def plot_mean_uppper_lower (fgure_for_plot, t_list_stand, S_list_stand, I_list_stand, finished_time, social_apply_time):\n",
    "    \n",
    "    S_mean = [np.mean(S_list_stand[i][1:]) for i in range(len(S_list_stand))]\n",
    "    S_lowerbound  = [np.quantile(S_list_stand[i][1:], 0.05) for i in range(len(S_list_stand))]\n",
    "    S_upperbound  = [np.quantile(S_list_stand[i][1:], 0.95) for i in range(len(S_list_stand))]\n",
    "    I_mean = [np.mean(I_list_stand[i][1:]) for i in range(len(I_list_stand))]\n",
    "\n",
    "    fi_in =t_list_stand.index(min(t_list_stand,key=lambda x:abs(x-finished_time)))\n",
    "\n",
    "    fgure_for_plot.plot(t_list_stand[:fi_in], S_mean[:fi_in], color = 'C0')\n",
    "    fgure_for_plot.fill_between(t_list_stand[:fi_in], S_lowerbound[:fi_in],S_upperbound[:fi_in], facecolor = 'C0', alpha=0.5,label='90% CI Total Confimred Cases' )\n",
    "    fgure_for_plot.scatter(t_list_stand[fi_in],  S_mean[fi_in], color ='C0', label = 'Total Confimred Cases \\n {} nodes (Mean)'.format(round( S_mean[fi_in],2)))\n",
    "    fgure_for_plot.axvline(x= np.median(social_apply_time), color= 'black', linestyle = '--', label = 'Social Distancing Starts at \\n  {} days (Mean)'.format(round(social_apply_time,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_for_country(number_node, num_neigbor, p_newman_po, family_size):\n",
    "    neigbor = num_neigbor\n",
    "    p_newman = p_newman_po\n",
    "    print(\"generating graph G with {} nodes {} neigbor and {} newconnect\".\n",
    "          format(number_node, neigbor, p_newman))\n",
    "\n",
    "    G_tem = nx.generators.random_graphs.watts_strogatz_graph(number_node, num_neigbor, p_newman_po)\n",
    "\n",
    "    for i in range(number_node):\n",
    "        G_tem._node[i].update({'name':i})\n",
    "        G_tem._node[i].update({'status':'S'})\n",
    "        G_tem._node[i].update({'From': -1})\n",
    "        G_tem._node[i].update({'S': 0})\n",
    "        G_tem._node[i].update({'E': -1})\n",
    "        G_tem._node[i].update({'I': -1})\n",
    "        G_tem._node[i].update({'R': -1})\n",
    "    \n",
    "    country_stable_edges_list = []\n",
    "    country_long_dis_edges_list = []\n",
    "    for i in G_tem.edges:\n",
    "        tem_index = int(i[0]/family_size)\n",
    "        if i[1] in  range(tem_index*family_size,tem_index*family_size+family_size):\n",
    "            country_stable_edges_list.append(i)\n",
    "        else:\n",
    "            country_long_dis_edges_list.append(i)\n",
    "\n",
    "    print(\"num orginal edges: \", len(G_tem.edges))\n",
    "    print(\"num baxic edges: \", len(country_stable_edges_list))\n",
    "    print(\"num long dis edges: \", len(country_long_dis_edges_list))\n",
    "    \n",
    "    return(G_tem,country_stable_edges_list, country_long_dis_edges_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation with the end of the second stage\n",
    "def SEIR_only_social_distancing(G,infrate, infected_rate, recoverey_rate,try_end_stage_one, \n",
    "                                stage_one_threshold, G_remove_list, max_time_simulation,quaran='F'):\n",
    "    # the whole recored\n",
    "    combine_t = []\n",
    "    combine_I = []\n",
    "    combine_E = []\n",
    "    combine_S = []\n",
    "    combine_R = []\n",
    "    return_statuses = ('S', 'E', 'I', 'R')\n",
    "    \n",
    "    # build the subgraph\n",
    "    H_pre = nx.DiGraph()\n",
    "    H_pre.add_node('S')\n",
    "    H_pre.add_edge('E', 'I', rate = infected_rate,)\n",
    "    H_pre.add_edge('I', 'R', rate = recoverey_rate)\n",
    "    J_pre = nx.DiGraph()\n",
    "    J_pre.add_edge(('I', 'S'), ('I', 'E'), rate = infrate)\n",
    "    J_pre.add_edge(('E', 'S'), ('E', 'E'), rate = infrate)\n",
    "    \n",
    "    \n",
    "    # set inital input variables\n",
    "    start_time = 0\n",
    "    end_time = try_end_stage_one \n",
    "    inital_status = nx.get_node_attributes(G, 'status')\n",
    "    num_node = len(G._node)\n",
    "    \n",
    "    final_cuc = 0\n",
    "    while final_cuc < round(stage_one_threshold*num_node)+1:\n",
    "        # run simulation in each time_interval before the first time threshold coming\n",
    "        sim_data =  EoN.Gillespie_simple_contagion(G, H_pre, J_pre, inital_status, return_statuses,\n",
    "                                                   tmin = start_time, tmax =end_time,return_full_data=True)\n",
    "        tem_cuc_list = [sim_data.I().tolist()[k]+sim_data.R().tolist()[k] for k in range(len(sim_data.I()))]\n",
    "        final_cuc = tem_cuc_list[-1]\n",
    "    \n",
    "    close_stage_1_time_index = tem_cuc_list.index(round(stage_one_threshold*num_node))+1\n",
    "    close_stage_1_time = sim_data.t()[close_stage_1_time_index-1]\n",
    "    \n",
    "    ## updata information of each node\n",
    "    update_time(sim_data, G, num_node, close_stage_1_time)\n",
    "    update_status(G, num_node)\n",
    "    update_from(sim_data ,G,close_stage_1_time)\n",
    "    \n",
    "    ## record the simiulation result\n",
    "    combine_t = combine_t + sim_data.t().tolist()[:close_stage_1_time_index]\n",
    "    combine_E = combine_E + sim_data.summary()[1]['E'].tolist()[:close_stage_1_time_index]\n",
    "    combine_S = combine_S + sim_data.S().tolist()[:close_stage_1_time_index]\n",
    "    combine_I = combine_I + sim_data.I().tolist()[:close_stage_1_time_index]\n",
    "    combine_R = combine_R + sim_data.R().tolist()[:close_stage_1_time_index]\n",
    "    G_1 =G.copy()\n",
    "    E_case_in_sd = combine_E[-1]\n",
    "\n",
    "\n",
    "    \n",
    "    ## The second stage\n",
    "    # Remove list from remove_list  \n",
    "    for i in G_remove_list:\n",
    "        G.remove_edge(*i)\n",
    "    \n",
    "    inital_status = nx.get_node_attributes(G, 'status')\n",
    "    start_time = close_stage_1_time\n",
    "    \n",
    "    #if quaran == \"T\":\n",
    "     #   J_pre.edges[(('E', 'S'), ('E', 'E'))]['rate']=0.032\n",
    "      #  J_pre.edges[(('I', 'S'), ('I', 'E'))]['rate']=0.032\n",
    "        \n",
    "    # run the simulation of the second stage after removing edges\n",
    "    sim_data =  EoN.Gillespie_simple_contagion(G, H_pre, J_pre, inital_status, return_statuses, tmin = start_time, \n",
    "                                               tmax = max_time_simulation, return_full_data = True)\n",
    "    \n",
    "    # updata information of each node\n",
    "    update_time(sim_data, G, num_node)\n",
    "    update_status(G, num_node)\n",
    "    update_from(sim_data ,G)\n",
    "\n",
    "    # record the simiulation result\n",
    "    combine_t = combine_t + sim_data.t().tolist()\n",
    "    combine_S = combine_S + sim_data.S().tolist()\n",
    "    combine_E = combine_E + sim_data.summary()[1]['E'].tolist()\n",
    "    combine_I = combine_I + sim_data.I().tolist()\n",
    "    combine_R = combine_R + sim_data.R().tolist()\n",
    "    \n",
    "    return(combine_t,combine_S, combine_E, combine_I,combine_R,close_stage_1_time, G_1, G,sim_data, E_case_in_sd )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SEIR_social_case_multi_run(G_tem, country_stable_edges_list, country_long_dis_edges_list, threshold_social_dis,\n",
    "                              policy_power, name, repeat_time, max_time_for_simulation):\n",
    "    # recored each iteration and the time of new infected node\n",
    "    wo_final_cumu = []\n",
    "    wo_infected_peak = [] \n",
    "    wo_infected_peak_time = []\n",
    "    wo_final_stop_time = []\n",
    "    wo_apply_social = []\n",
    "    wo_exposed_number_in_social = []\n",
    "\n",
    "    arr_record_cumu = []\n",
    "    arr_record_I = []\n",
    "    arr_record_R = []\n",
    "\n",
    "    label_plot = ['I', 'R','Cumu']\n",
    "    \n",
    "    ## create a same timeline for all simualtions of benchmark\n",
    "    pre_t_standard = [d for d in np.arange(0, max_time_for_simulation+0.2, 0.2)]\n",
    "    arr_record_cumu = []\n",
    "    arr_record_I = []\n",
    "    arr_record_R = []\n",
    "    \n",
    "    ## recored all run result in each time point\n",
    "    for i in pre_t_standard:\n",
    "        arr_record_cumu.append([i])\n",
    "        arr_record_I.append([i])\n",
    "        arr_record_R.append([i])\n",
    "\n",
    "    # reocrd the time\n",
    "    for i in range(repeat_time):\n",
    "        G = G_tem.copy()\n",
    "        N = len(G_tem.nodes())\n",
    "        initial_infected_list = random.sample(range(N), round(N*rho_set))\n",
    "        for i in initial_infected_list:\n",
    "            G._node[i]['I'] = 0\n",
    "            G._node[i]['status'] = 'I'\n",
    "            G._node[i]['From'] = None\n",
    "\n",
    "        prob_edges_remove =policy_power\n",
    "        # random pick remove list from non-neighbor edages   \n",
    "        G_country_remove_list = random.sample(country_long_dis_edges_list, \n",
    "                                              round(len(country_long_dis_edges_list)*prob_edges_remove))\n",
    "\n",
    "        result = SEIR_only_social_distancing(G,beta, tau ,gamma,40,threshold_social_dis, \n",
    "                                             G_country_remove_list, max_time_for_simulation,quaran=\"T\")\n",
    "\n",
    "        pre_t = result[0]\n",
    "        pre_S = result[1]\n",
    "        pre_I = result[3]\n",
    "        pre_R = result[4]\n",
    "        result_stage = result[5]\n",
    "        \n",
    "        infected_peak = max(pre_I)\n",
    "        infected_peak_time = pre_t[pre_I.index(infected_peak)]\n",
    "        final_stop_time  = pre_t[-1]\n",
    "\n",
    "        # combine all data following the time line\n",
    "        index_time_line = [pre_t.index(min(pre_t, key=lambda x:abs(x-i))) for i in pre_t_standard] \n",
    "        pre_I_standard = [pre_I[i] for i in index_time_line]\n",
    "        pre_R_standard = [pre_R[i] for i in index_time_line]\n",
    "\n",
    "        for i in range(len(pre_t_standard)):\n",
    "            arr_record_cumu[i].append(pre_I_standard[i]+pre_R_standard[i])\n",
    "            arr_record_I[i].append(pre_I_standard[i])\n",
    "            arr_record_R[i].append(pre_R_standard[i])\n",
    "\n",
    "        wo_final_cumu.append(pre_I[-1]+pre_R[-1] ) \n",
    "        wo_infected_peak.append(infected_peak)\n",
    "        wo_infected_peak_time.append(infected_peak_time) \n",
    "        wo_final_stop_time.append(final_stop_time)\n",
    "        wo_apply_social.append(result_stage)\n",
    "        wo_exposed_number_in_social.append(result[9])\n",
    "\n",
    "\n",
    "#     fig = plt.figure(figsize = (6,4))\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.spines['right'].set_visible(False)\n",
    "#     ax.spines['top'].set_visible(False)\n",
    "#     plot_mean_uppper_lower (ax, pre_t_standard, arr_record_cumu, arr_record_I, \n",
    "#                             max_time_for_simulation,np.mean(wo_apply_social))\n",
    "\n",
    "  \n",
    "#     ax.set_xlabel(\"Time\",fontsize=14)\n",
    "#     ax.set_ylabel(\"Cases\",fontsize=14)\n",
    "#     ax.set_title(\"{}: apply {} policy power when {}% cumulative cases\".format(name,round(policy_power*100), round(threshold_social_dis*100,2)))\n",
    "#     #plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "#     plt.legend(loc='lower right',fontsize = 11)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "    print(\"The cumulative case: \", mean_var_std(wo_final_cumu))\n",
    "#     print(\"The infected peak: \", mean_var_std(wo_infected_peak))\n",
    "#     print(\"The time of infected peak:\", mean_var_std(wo_infected_peak_time))\n",
    "#     print(\"The time of policy application:\", mean_var_std(wo_apply_social))\n",
    "#     print(\"The exposed case when policy application:\", mean_var_std(wo_exposed_number_in_social))\n",
    "        \n",
    "    return(pre_t_standard, arr_record_cumu, arr_record_I,wo_apply_social,wo_exposed_number_in_social)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of nodes\n",
    "N =10000 \n",
    "\n",
    "# initial infected fraction, 1 agent\n",
    "rho_set = 0.0001\n",
    "beta = 0.019\n",
    "\n",
    "#transmission rate\n",
    "tau = 1/7\n",
    "# the expected connecting time with neighbor before infected is 1/tau  \n",
    "# the variance is 1/(tau*tau)\n",
    "\n",
    "#recovery rate\n",
    "gamma = 1/14\n",
    "# the expected recovered time after infected is 1/gamma \n",
    "# the varance is 1/(gamam*gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact-based Networking Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each country, the contact-based network is constructed by the number of the mean reported contacts and the average household size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Austria\n",
    "# preditcte 13.2 mean reported contacts and 2.27 average household size\n",
    "G_Aus, Aus_stable_edges_list, Aus_long_dis_edges_list = G_for_country(N, 14,0.35,3)\n",
    "    \n",
    "G_play = G_Aus.copy()\n",
    "for i in Aus_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size\")\n",
    "print(mean_var_std(G_subgraph_node_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hungary\n",
    "# preditcte 12.56 mean reported contacts and 2.60 average household size\n",
    "G_Hun, Hun_stable_edges_list, Hun_long_dis_edges_list = G_for_country(N, 12,0.23,3)\n",
    "    \n",
    "G_play = G_Hun.copy()\n",
    "for i in Hun_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size\")\n",
    "print(mean_var_std(G_subgraph_node_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poland\n",
    "# 14.40 mean reported contacts and 2.81 average household size\n",
    "G_Poland, Poland_stable_edges_list, Poland_long_dis_edges_list = G_for_country(N, 14,0.12,3)\n",
    "mean_var_std([G_Poland.degree(i) for i in range(N)])\n",
    "G_play = G_Poland.copy()\n",
    "for i in Poland_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size\")\n",
    "print(mean_var_std(G_subgraph_node_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slovenia\n",
    "# preditcte 12.40 mean reported contacts and 2.47 average household size\n",
    "G_Slo, Slo_stable_edges_list, Slo_long_dis_edges_list = G_for_country(N, 12,0.27,3)\n",
    "    \n",
    "G_play = G_Slo.copy()\n",
    "for i in Slo_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size\")\n",
    "print(mean_var_std(G_subgraph_node_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slovakia\n",
    "# preditcte 13.8 mean reported contacts and 2.80average household size\n",
    "G_Slovakia, Slovakia_stable_edges_list, Slovakia_long_dis_edges_list = G_for_country(N, 14,0.15,3)\n",
    "    \n",
    "G_play = G_Slovakia.copy()\n",
    "for i in Slovakia_long_dis_edges_list:\n",
    "    G_play.remove_edge(*i)\n",
    "G_subgraph = [i for i in nx.connected_components(G_play)]\n",
    "G_subgraph_node_list  = [len(i) for i in G_subgraph]\n",
    "print(\"The distribution of household size\")\n",
    "print(mean_var_std(G_subgraph_node_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy_Power_check = pd.read_csv('GSI_and_cases_track_validation_set')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Simulatie the pandenic of countries in the first 60 days. All modified SEIR models using in the following simulaiton are shown in the function section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of run\n",
    "tst = 500\n",
    "## time of each run\n",
    "tim  = 60\n",
    "Aus_social_dis = SEIR_social_case_multi_run(G_Aus, Aus_stable_edges_list, Aus_long_dis_edges_list, 0.0010,\n",
    "                                  0.8148,'Austria', tst , tim)\n",
    "\n",
    "Slo_social_dis = SEIR_social_case_multi_run(G_Slo, Slo_stable_edges_list, Slo_long_dis_edges_list, 0.0014,\n",
    "                                            0.8981,'Slovenia', tst , tim)\n",
    "\n",
    "poland_social_dis = SEIR_social_case_multi_run(G_Poland, Poland_stable_edges_list, Poland_long_dis_edges_list, 0.0005,\n",
    "                                  0.8333,'Poland', tst , tim)\n",
    "\n",
    "Hun_social_dis = SEIR_social_case_multi_run(G_Hun, Hun_stable_edges_list, Hun_long_dis_edges_list, 0.0003,\n",
    "                                            0.7685,'Hungary', tst , tim)\n",
    "\n",
    "Slovakia_social_dis = SEIR_social_case_multi_run(G_Slovakia, Slovakia_stable_edges_list, Slovakia_long_dis_edges_list, 0.0002,\n",
    "                                            0.7500,'Slovakia', tst , tim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
